{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFqw_Y0ptXPk"
      },
      "source": [
        "## App Quickstart"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ex9RTutztXPk"
      },
      "source": [
        "1. Install Python 3.10 if not already installed.\n",
        "\n",
        "2. Clone the `chatgpt-retrieval-plugin` repository:\n",
        "\n",
        "```\n",
        "git clone git@github.com:openai/chatgpt-retrieval-plugin.git\n",
        "```\n",
        "\n",
        "_**Note**: To see how we setup the *hosted app* on DigitalOcean [refer to this video](https://youtu.be/hpePPqKxNq8), otherwise continue to setup the app locally by following the remaining steps._\n",
        "\n",
        "3. Navigate to the app directory:\n",
        "\n",
        "```\n",
        "cd /path/to/chatgpt-retrieval-plugin\n",
        "```\n",
        "\n",
        "4. Install `poetry`:\n",
        "\n",
        "```\n",
        "pip install poetry\n",
        "```\n",
        "\n",
        "5. Create a new virtual environment:\n",
        "\n",
        "```\n",
        "poetry env use python3.10\n",
        "```\n",
        "\n",
        "6. Install the `retrieval-app` dependencies:\n",
        "\n",
        "```\n",
        "poetry install\n",
        "```\n",
        "\n",
        "7. Set app environment variables:\n",
        "\n",
        "* `BEARER_TOKEN`: Secret token used by the app to authorize incoming requests. We will later include this in the request `headers`. The token can be generated however you prefer, such as using [jwt.io](https://jwt.io/).\n",
        "\n",
        "* `OPENAI_API_KEY`: The OpenAI API key used for generating embeddings with the `text-embedding-ada-002` model. [Get an API key here](https://platform.openai.com/account/api-keys)!\n",
        "\n",
        "8. Set Pinecone-specific environment variables:\n",
        "\n",
        "* `DATASTORE`: set to `pinecone`.\n",
        "\n",
        "* `PINECONE_API_KEY`: Set to your Pinecone API key. This requires a free Pinecone account and can be [found in the Pinecone console](https://app.pinecone.io/).\n",
        "\n",
        "* `PINECONE_ENVIRONMENT`: Set to your Pinecone environment, looks like `us-east1-gcp`, `us-west1-aws`, and can be found next to your API key in the [Pinecone console](https://app.pinecone.io/).\n",
        "\n",
        "* `PINECONE_INDEX`: Set this to your chosen index name. The name you choose is your choice, we just recommend setting it to something descriptive like `\"openai-retrieval-app\"`. *Note that index names are restricted to alphanumeric characters, `\"-\"`, and can contain a maximum of 45 characters.*\n",
        "\n",
        "8. Run the app with:\n",
        "\n",
        "```\n",
        "poetry run start\n",
        "```\n",
        "\n",
        "If running the app locally you should see something like:\n",
        "\n",
        "```\n",
        "INFO:     Uvicorn running on http://0.0.0.0:8000\n",
        "INFO:     Application startup complete.\n",
        "```\n",
        "\n",
        "In that case, the app has automatically connected to our index (specified by `PINECONE_INDEX`), if no index with that name existed beforehand, the app creates one for us.\n",
        "\n",
        "Now we're ready to move on to populating our index with some data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVsnYnVJtXPl"
      },
      "source": [
        "## Required Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUrzZ35ztXPl"
      },
      "source": [
        "There are a few Python libraries we must `pip install` for this notebook to run, those are:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vZ-IYr9tXPl",
        "outputId": "39a6b998-dcce-4dda-a4b0-112c39e47795"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain tiktoken tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgZqGDV6tXPm"
      },
      "source": [
        "## Preparing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "EfhjpaKktXPm",
        "outputId": "1ba0a053-7099-485c-88b4-e2396f77d324"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "[Document(page_content='It is only after years of preparation that the young artist should touch color– not color used descriptively, that is, but as a means of personal expression.\\n\\nHenri Matisse\\n\\nFor a long time I limited myself to one color – as a form of discipline.\\n\\nPablo Picasso\\n\\nColor Image Processing\\n\\nChapter 6 어\\n\\nPreview } Motivation of using colors in image processing\\n\\n} A powerful descriptor that simplifies object identification } Thousands of color shades and intensities can be discerned by\\n\\nhumans\\n\\n} Two major areas in color image processing\\n\\n} Full\\n\\n\\n\\ncolor processing\\n\\n} images are acquired with a full-color sensor\\n\\n} Pseudo\\n\\n\\n\\ncolor processing\\n\\n} assigning a color to a particular monochrome intensity or\\n\\nrange of intensities\\n\\n} Full-color image processing techniques are used in broad\\n\\nrange of applications, as the hardware cost is getting reasonable\\n\\n2\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\nPreview\\n\\n} This chapter covers } Color fundamentals } Color models } Pseudocolor image processing } Basics of full-color image processing } Color transformations } Color image smoothing and sharpening } Using color in image segmentation } Noise in color images } Color image compression\\n\\n3\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.1 Color fundamentals\\n\\n} The process followed by the human brain in perceiving\\n\\nand interpreting color is a physiopsychological phenomenon that is not yet fully understood\\n\\n} Sir Issac Newton – color spectrum } The colors that humans and some other animals perceive in an object are determined by the nature of the light reflected from the object\\n\\nFigure 6.1 Color spectrum seen by passing white light through a prism. (Courtesy of the General Electric Co., Lighting Division.)\\n\\n4\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.1 Color fundamentals } Characterization of light is central to the science of color\\n\\n} Achromatic: the only attribute is its intensity\\n\\n} Chromatic light: the electromagnetic spectrum\\n\\nfrom approximately 400 – 700 %&\\n\\nFigure 6.2 Wavelengths comprising the visible range of the electromagnetic spectrum. (Courtesy of the General Electric Co., Lighting Division.)\\n\\n} Three basic quantities used to describe the quality of a chromatic light source: radiance, luminance, and brightness\\n\\n5\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.1 Color fundamentals } Radiance\\n\\n} The total amount of energy that flows from the light source } It is measured in watts (!)\\n\\n} Luminance\\n\\n} A measure of the amount of energy an observer perceives\\n\\nfrom a light source\\n\\n} It is measured in lumens (\"#)\\n\\n} Brightness\\n\\n} A subjective descriptor that is practically impossible to\\n\\nmeasure\\n\\n} It embodies the achromatic notion of intensity and\\n\\nis one of the key factors in describing color sensation\\n\\n6\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.1 Color fundamentals\\n\\n} Due to the absorption characteristics of the human eye, colors are seen as variable combinations of the primary colors } red (700 &#), green (546.1 &#), and blue (435.8 &#)\\n\\nFigure 6.3 Absorption of light by the red, green, and blue cones in the human eye as a function of wavelength.\\n\\n7\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.1 Color fundamentals\\n\\n} The secondary colors of light\\n\\n} magenta (. + 0), cyan (1 + 0), yellow (. + 1) } Differentiating between the primary colors of light and the\\n\\nprimary colors of pigments or colorants is important\\n\\nFigure 6.4 Primary and secondary colors of light and pigments. (Courtesy of the General Electric Co., Lighting Division.)\\n\\n8\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.1 Color fundamentals } Color TV reception is an example of the additive nature\\n\\nof light colors\\n\\n} The characteristics used to distinguish one color from\\n\\nanother are brightness, hue, and saturation } Hue\\n\\n} An attribute associated with the dominant wavelength in a mixture of light waves; it represents dominant color as perceived by an observer\\n\\n} When we call an object red, orange, or yellow, we are specifying its\\n\\nhue } Saturation\\n\\n} The relative purity or the amount of white light mixed with a hue } The pure spectrum colors are fully saturated } The degree of saturation is inversely proportional to the amount of\\n\\nwhite light added\\n\\n} Chromaticity: hue and saturation\\n\\n9\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.1 Color fundamentals\\n\\n} Tristimulus values\\n\\n} The amounts of red (2), green (3), and blue (4) needed to\\n\\nform any particular color\\n\\n} A color is specified by its trichromatic coefficients (F6.5)\\n\\n! =\\n\\n# # + % + &\\n\\n,\\n\\n( =\\n\\n% # + % + &\\n\\n,\\n\\n) =\\n\\n& # + % + &\\n\\n→ ! + ( + ) = 1\\n\\n} Another approach for specifying colors\\n\\n– CIE chromaticity diagram } It shows color composition as a function of ! (red) and \" (green) } The corresponding value of # (blue) is obtained from the equation\\n\\n! = 1 − % − &\\n\\n10\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.1 Color fundamentals\\n\\n} The chromaticity diagram:\\n\\n} useful for color mixing because a straight-line segment joining any two\\n\\npoints in the diagram defines all the different color variations that can be obtained by combining these two colors additively\\n\\n} A typical range of colors (the color gamut) by RGB monitors and\\n\\nprinting devices\\n\\nFigure 6.5 The C I E chromaticity diagram. (Courtesy of the General Electric Co., Lighting Division.)\\n\\nFigure 6.6 Illustrative color gamut of color monitors (triangle) and color printing devices (shaded region).\\n\\n11\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.1 Color fundamentals } 3-CCD/CMOS camera\\n\\nhttps://mvpromedia.com/wp\\n\\n\\n\\ncontent/uploads/2020/02/JAI\\n\\n\\n\\nPress\\n\\n\\n\\nPhoto_Fusion\\n\\n\\n\\nSeries_FS\\n\\n\\n\\n3200T\\n\\n\\n\\n10GE\\n\\n\\n\\nPrism\\n\\n\\n\\nIllus tration_2\\n\\n\\n\\nJPEG\\n\\n\\n\\nformat\\n\\n\\n\\nscaled.jpg\\n\\n12\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.1 Color fundamentals } Bayer Pattern\\n\\n} Color interpolation\\n\\n13\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\nhttps://www.1stvision.com/machine\\n\\n\\n\\nvision\\n\\n\\n\\nsolutions/wp\\n\\n\\n\\ncontent/uploads/2018/04/b ayer\\n\\n\\n\\nvs\\n\\n\\n\\n3cmos\\n\\n\\n\\nprism.png\\n\\n6.2 Color models\\n\\n} The purpose of a color model\\n\\n} To facilitate the specification of colors in some standard way } A specification of a coordinate system and a subspace within that system where each color is represented by a single point\\n\\n} RGB: for color monitors and a broad class of color video\\n\\ncameras\\n\\n} CMY (cyan, magenta, yellow) and CMYK (+black) for color\\n\\nprinting\\n\\n} HSI (hue, saturation, intensity)\\n\\n} corresponds to the way humans describe and interpret color } it decouples the color and gray-scale information in an image, making it suitable for many of the gray-scale techniques developed in this book\\n\\n} Numerous color models\\n\\n14\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.2 Color models\\n\\n} The RGB color model\\n\\n} Based on a Cartesian coordinate system } The different colors are points on or inside the cube, and are\\n\\ndefined by vector extending from the origin\\n\\n} Pixel depth: the number of bits used to represent each pixel\\n\\nin RGB space } A depth of 24 bits: an RGB image in which each of the red, green, and\\n\\nblue images is an 8-bit image (full-color: 16,777,216)\\n\\nFigure 6.7 Schematic of the RGB color cube. Points along the main diagonal have gray values, from black at the origin to white at point (1, 1, 1).\\n\\nFigure 6.8 A 24\\n\\n\\n\\nbit RGB color cube.\\n\\n15\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.2 Color models\\n\\n} The RGB color model\\n\\n} Ex. 6.1 (generating a cross-section of the RGB color cube and\\n\\nits three hidden planes)\\n\\nFigure 6.9 (a) Generating the RGB image of the cross-sectional color plane (127, G, B). (b) The three hidden surface planes in the color cube of Fig. 6.8.\\n\\n16\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.2 Color models\\n\\n} The CMY and CMYK color models\\n\\n} Most devices that deposit colored pigments on paper, such as\\n\\ncolor printers and copiers, require CMY data input or perform an RGB to CMY conversion internally\\n\\n\\' ( )\\n\\n=\\n\\n1 1 1\\n\\n−\\n\\n+ ,\\n\\n} Equal amount of the pigment primaries, 5, 6, and 3 should\\n\\nproduce black } Combining these colors for printing produces a muddy-looking black } In order to produce true black, a fourth color, black, is added, giving\\n\\nrise to the CMYK color model\\n\\n17\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.2 Color models\\n\\n} The CMY and CMYK color models\\n\\n} Pixelwise operation\\n\\n} Conversion from CMY to CMYK\\n\\n} $ = min(*, ,, -) } If $ = 1 pure black , * = , = - = 0 Otherwise 1 − $ * = * − $, * =\\n\\n1 − $ , = , − $, , =\\n\\n!\"# $\"# %\"# $\"#\\n\\n1 − $\\n\\n\\n\\n=\\n\\n\\n\\n− $,\\n\\n=\\n\\n&\"# $\"#\\n\\n¨ In the rage [0, 1]\\n\\n} Conversion from CMYK to CMY\\n\\n= * 1 − $ + $ , = , 1 − $ + $ - = - 1 − $ + $\\n\\n18\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.2 Color models\\n\\n} The HSI color model\\n\\n} The model decouples the intensity component from the color- carrying information (hue and saturation) in a color image } The model is an ideal tool for developing image processing algorithms based on color descriptions that are natural and intuitive to humans\\n\\n} Extracting intensity from an RGB image\\n\\nFigure 6.10 Conceptual relationships between the RGB and HSI color models.\\n\\n19\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.2 Color models\\n\\n} The HSI color model\\n\\nFigure 6.11 Hue and saturation in the HSI color model. The dot is any color point. The angle from the red axis gives the hue. The length of the vector is the saturation. The intensity of all colors in any of these planes is given by the position of the plane on the vertical intensity axis.\\n\\n20\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\nFigure 6.12 The HSI color model based on (a) triangular, and (b) circular color planes. The triangles and circles are perpendicular to the vertical intensity axis.\\n\\n6.2 Color models } The HSI color model\\n\\n} Converting colors from RGB to HSI\\n\\n} ! = #\\n\\n$, 360 − $,\\n\\nif ( ≤\\n\\n\\n\\nif ( >\\n\\n\\n\\n0ℎ232 $ = cos!\"\\n\\n[ $!% &($!()]\\n\\n! \" $!% \"&($!()(%!()\\n\\n} 7 = 1 −\\n\\n+\\n\\n$&%&(\\n\\nmin ;,\\n\\n\\n\\n, (\\n\\n} < =\\n\\n\"\\n\\n+\\n\\n(; +\\n\\n\\n\\n+ ()\\n\\n} Converting colors from HSI to RGB\\n\\n} There are three sectors of interest, corresponding to the 120° intervals in the\\n\\nseparation of primaries\\n\\n} RG sector (0∘ ≤ ! ≤ 120∘)\\n\\n¨ \\' = ) 1 − , , . = ) 1 +\\n\\n! \"#$ % \"#$ &\\'°(%\\n\\n, 0 = 3) − (. + \\')\\n\\n} GB sector (120∘ ≤ ! ≤ 240∘) ! = ! − 120°\\n\\n¨ . = ) 1 − , , 0 = ) 1 +\\n\\n! \"#$ % \"#$ &\\'°(%\\n\\n, \\' = 3) − (. + 0)\\n\\n} BR sector (240∘ ≤ ! ≤ 360∘) ! = ! − 240°\\n\\n¨ 0 = ) 1 − , , \\' = ) 1 +\\n\\n! \"#$ % \"#$ &\\'°(%\\n\\n, . = 3) − (0 + \\')\\n\\n21\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.2 Color models\\n\\n} The HSI color model\\n\\n} Converting colors from HSI to RGB\\n\\n} Ex. 6.2 (the HSI values corresponding to the image of the RGB color\\n\\ncube)\\n\\nFigure 6.13 HSI components of the image in Fig. 6.8: (a) hue, (b) saturation, and (c) intensity images.\\n\\n22\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.2 Color models\\n\\n} Manipulating HSI component images\\n\\nFigure 6.14 (a) RGB image and the components of its corresponding HSI image: (b) hue, (c) saturation, and (d) intensity.\\n\\nFigure 6.15 (a)-(c) Modified HSI component images. (d) Resulting RGB image. (See Fig. 6.14 for the original HSI images.)\\n\\n23\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.2 Color models } A device independent color model\\n\\n} In conjunction with digital cameras, flatbed scanner, and inkjet\\n\\nprinter, a PC can be turned into a digital darkroom\\n\\n} A device independent color model\\n\\n} Relates the color gamuts of the monitors and output devices } CIELAB\\n\\n,∗ = 116 . ℎ\\n\\n1∗ = 500 ℎ\\n\\n4∗ = 200 ℎ\\n\\n% %/ # #/ 0 0#\\n\\n− 16\\n\\n− ℎ\\n\\n− ℎ\\n\\n% %/ 1 1#\\n\\n,\\n\\n6ℎ787 ℎ(:) = <\\n\\n$ :,\\n\\n: > 0.008856 7.787: + 16/116, : ≤ 0.008856\\n\\n} While not a directly displayable format, its gamut encompasses the\\n\\nentire visible spectrum and can represent accurately the colors of any device\\n\\n24\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.3 Pseudocolor image processing\\n\\n} Pseudocolor (false color)\\n\\nimage processing consists of assigning colors to gray values based on a specified criterion\\n\\n} For human visualization and interpretation of gray-scale events in an\\n\\nimage or sequence of image\\n\\n} Intensity slicing and color coding\\n\\n} >(!, \") = ?4 to slice the image function into two levels (Fig. 6.16) if 3 %, & ∈ 5! 3 %, & = 6!\\n\\n} Alternate representation (Fig. 6.17)\\n\\nFigure 6.16 Graphical interpretation of the intensity slicing technique.\\n\\nFigure 6.17 An alternative representation of the intensity slicing technique.\\n\\n25\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.3 Pseudocolor image processing\\n\\n} Intensity slicing and color coding\\n\\n} Ex. 6.3 (intensity slicing and color coding)\\n\\n} Regions that appear of constant intensity in the monochrome image are quite variable. The color image shows eight different regions of constant intensity\\n\\nFigure 6.18 (a) Grayscale image of the Picker Thyroid Phantom. (b) Result of intensity slicing using eight colors. (Courtesy of Dr. J. L. Blankenship, Oak Ridge National Laboratory.)\\n\\n26\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.3 Pseudocolor image processing\\n\\n} Intensity slicing and color coding\\n\\n} Subdivision of gray scale based on physical characteristics\\n\\nof the image\\n\\nFigure 6.19 (a) X-ray image of a weld. (b) Result of color coding. (Original image courtesy of X-T E K Systems, Ltd.)\\n\\n27\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.3 Pseudocolor image processing\\n\\n} Intensity slicing and color coding\\n\\n} Ex. 6.4 (use of color to highlight rainfall levels)\\n\\nFigure 6.20 (a) Grayscale image in which intensity (in the horizontal band shown) corresponds to average monthly rainfall. (b) Colors assigned to intensity values. (c) Color-coded image. (d) Zoom of the South American region. (Courtesy of NASA.)\\n\\n28\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.3 Pseudocolor image processing\\n\\n} Intensity to color transformations\\n\\n} Three independent transformations on the gray level\\n\\nof any input pixel } The method discussed in the previous section is\\n\\na special case of the technique\\n\\nFigure 6.21 Functional block diagram for pseudocolor image processing. Images 2\", 2#, and 2$ are fed into the corresponding red, green, and blue inputs of an RGB color monitor.\\n\\n29\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.3 Pseudocolor image processing\\n\\n} Intensity to color transformations\\n\\n} Ex. 6.5 (using pseudocolor to highlight explosives in X-ray\\n\\nimages) : an airport X-ray scanning system\\n\\nFigure 6.22 Pseudocolor enhancement by using the gray level to color transformations in Fig. 7.23. (Original image courtesy of Dr. Mike Hurwitz, Westinghouse.)\\n\\nFigure 6.23 Transformation functions used to obtain the pseudocolor images in Fig. 6.22.\\n\\n30\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.3 Pseudocolor image processing\\n\\n} Intensity to color transformations\\n\\n} Multispectral image processing\\n\\n} Different sensors produce individual monochrome images,\\n\\neach in a different spectral band\\n\\n} Additional processing\\n\\n} color balancing, combining images, and selecting the three images for\\n\\ndisplay\\n\\nFigure 6.24 A pseudocolor coding approach using multiple grayscale images. The inputs are grayscale images. The outputs are the three components of an RGB composite image.\\n\\n31\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.3 Pseudocolor image processing\\n\\n} Intensity to color transformations\\n\\n} Ex. 6.6 (color coding of multispectral images)\\n\\nFigure 6.25 (a)–(d) Red (R), green (G), blue (B), and near-infrared (IR) components of a LANDSAT multispectral image of the Washington, D.C. area. (e) RGB color composite image obtained using the IR, G, and B component images. (f) RGB color composite image obtained using the R, IR, and B component images. (Original multispectral images courtesy of NASA.)\\n\\n32\\n\\n6. Color Image Processing\\n\\nFigure 6.26 (a) Pseudocolor rendition of Jupiter Moon Io. (b) A close-up. (Courtesy of NASA.)\\n\\nSpring 2023\\n\\n6.4 Basics of full\\n\\n\\n\\ncolor image processing\\n\\n} Two major categories of full-color image processing methods\\n\\n} Processing each component image individually and forming a\\n\\ncomposite processed color image from the individually processed components\\n\\n} Working with color pixels directly\\n\\n} Color pixels are vectors\\n\\n#! #\" ##\\n\\n} ! =\\n\\n#!(\\', )) #\"(\\', )) ##(\\', )) } In order for per-color-component and vector-based processing\\n\\n$(\\', )) %(\\', )) &(\\', ))\\n\\n, ! \\', ) =\\n\\n$ % &\\n\\n=\\n\\n=\\n\\nto be equivalent, two conditions have to be satisfied } The process has to be applicable to both vectors and scalars } The operation on each component of a vector must be independent\\n\\nof the other components\\n\\n33\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.4 Basics of full\\n\\n\\n\\ncolor image processing\\n\\n} The process of neighborhood averaging\\n\\n1.\\n\\n2.\\n\\naveraging would be accomplished by summing the gray levels of all the pixels in the 2D neighborhood and dividing by the total number of pixels in the neighborhood averaging would be done by summing all the vectors in the 3D neighborhood and dividing each component by the total number of vectors in the neighborhood\\n\\nFigure 6.27 Spatial neighborhoods for grayscale and RGB color images. Observe in (b) that a single pair of spatial coordinates, (3, 5), addresses the same spatial location in all three images.\\n\\n34\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.5 Color transformations\\n\\n} Processing the components of a color image within the context of a\\n\\nsingle color model (as opposed to the conversion of those components between models)\\n\\n} Formulation\\n\\n} The pixel values here are triplets or quartets from the color space chosen to represent the images\\n\\n} HSI color space\\n\\nC6 = D6 86 , E = 1, … , G, H !, ( = D I !, ( } RGB: 7\" = 89\", : = 1,2,3\\n\\nFigure 6.28 A full-color image and its various color- space components. (Original image courtesy of MedData Interactive.)\\n\\n35\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.5 Color transformations\\n\\n} Formulation\\n\\n} CMY/CMYK color space\\n\\nFigure 6.29 Adjusting the intensity of an image using color transformations. (a) Original image. (b) Result of decreasing its intensity by 30% (i.e., letting 7 = 0.7). (c) The required RGB mapping function. (d)–(e) The required CMYK mapping functions. (f) The required CMY mapping function. (g)–(h) The required HSI mapping functions. (Original image courtesy of MedData Interactive.)\\n\\n36\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.5 Color transformations\\n\\n} Color complements\\n\\n} Complements: the hues directly opposite one another on the\\n\\ncolor circle\\n\\n} Analogous to the gray-scale negatives\\n\\nFigure 6.30 Color complements on the color circle.\\n\\n37\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.5 Color transformations\\n\\n} Color complements\\n\\n} Ex. 6.7 (computing color image complements)\\n\\nFigure 6.31 Color complement transformations. (a) Original image. (b) Complement transformation functions. (c) Complement of (a) based on the RGB mapping functions. (d) An approximation of the RGB complement using HSI transformations.\\n\\n38\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.5 Color transformations } Color slicing\\n\\n} Highlighting a specific range of colors in an image is useful for\\n\\nseparating objects from their surroundings } To display the colors of interest so that they stand out from the\\n\\nbackground or\\n\\n} To use the region defined by the colors as a mask for further\\n\\nprocessing\\n\\n} One simple way to slice a color image\\n\\n} The transformations highlight the colors around the prototype\\n\\nby forcing all other colors to the midpoint of the reference color space\\n\\n¨ 7\" = =\\n\\n0.5,\\n\\n9\",\\n\\nif\\n\\n9# − @# >\\n\\n$ % &\\'( )\\n\\n\\n\\n#\\n\\n\\n\\n\\' otherwise\\n\\n,\\n\\n: = 1, … , C\\n\\n¨ 7\" = =\\n\\n0.5, 9\",\\n\\n\\' if ∑#+)\\n\\n9# − @#\\n\\n%\\n\\n% , >\\n\\n\\n\\n, otherwise\\n\\n: = 1, … , C\\n\\n39\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.5 Color transformations\\n\\n} Color slicing\\n\\n} Ex. 6.8 (color slicing)\\n\\nFigure 6.32 Color-slicing transformations that detect (a) reds within an RGB cube of width < = 0.2549 centered at (0.6863, 0.1608, 0.1922), and (b) reds within an RGB sphere of radius 0.1765 centered at the same point. Pixels outside the cube and sphere were replaced by color (0.5, 0.5, 0.5).\\n\\n40\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.5 Color transformations\\n\\n} Tone and color corrections\\n\\n} The principal benefit of calibrated imaging systems\\n\\n} They allow tonal and color imbalances to be corrected interactively\\n\\n} The tonal range of an image refers to its general distribution of\\n\\ncolor intensities } High-key images: concentrated at high intensities } Low-key images: located predominantly at low intensities } Middle-key images: lie in between\\n\\n41\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.5 Color transformations\\n\\n} Tone and color corrections } Ex. 6.9 (tonal transformation) } Tonal corrections for flat, light,\\n\\nand dark color images\\n\\nFigure 6.33 Tonal corrections for flat, light (high key), and dark (low key) color images. Adjusting the red, green, and blue components equally does not always alter the image hues significantly.\\n\\n42\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.5 Color transformations\\n\\n} Tone and color corrections } Ex. 6.10 (color balancing)\\n\\n} Color balancing corrections\\n\\nfor CMYK color images\\n\\nFigure 6.34 Color balancing a C M Y K image.\\n\\n43\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.5 Color transformations\\n\\n} Histogram processing of color images\\n\\n} It is generally unwise to histogram equalize the components of a color image independently (resulting in erroneous color)\\n\\n} A more logical approach is to spread the color intensities uniformly,\\n\\nleaving the colors themselves (e.g., hues) unchanged\\n\\n} Ex. 6.11 (HE in the HSI color\\n\\nspace)\\n\\nFigure 6.35 Histogram equalization (followed by saturation adjustment) in the HSI color space.\\n\\n44\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.6 Color image smoothing and sharpening\\n\\n} The basics of neighborhood processing are illustrated\\n\\nwithin the context of color image smoothing and sharpening\\n\\n} Color image smoothing\\n\\n} Smoothing by neighborhood averaging can be carried out on a\\n\\nper\\n\\n\\n\\ncolor\\n\\n\\n\\nplane basis\\n\\n̅M %, & =\\n\\n)\\n\\n\\n\\n∑ .,0 ∈2)\\n\\n\\n\\nM(7, O) =\\n\\n)\\n\\n)\\n\\n)\\n\\n\\n\\n∑ .,0 ∈2)\\n\\n\\n\\n∑ .,0 ∈2)\\n\\n\\n\\n∑ .,0 ∈2)\\n\\n\\n\\n(7, O)\\n\\n+(7, O)\\n\\n,(7, O)\\n\\n45\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.6 Color image smoothing and sharpening\\n\\n} Color image smoothing\\n\\n} Ex. 6.12 (color image smoothing by neighborhood averaging)\\n\\nFigure 6.36 (a) RGB image. (b) Red component image. (c)Green component. (d) Blue component.\\n\\n46\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.6 Color image smoothing and sharpening\\n\\n} Color image smoothing\\n\\n} Ex. 6.12 (color image smoothing by neighborhood averaging)\\n\\nFigure 6.37 HSI components of the RGB color image in Fig. 6.36(a). (a) Hue. (b) Saturation. (c) Intensity.\\n\\nFigure 6.38 Image smoothing with a 5×5 averaging kernel. (a) Result of processing each RGB component image. (b) Result of processing the intensity component of the HSI image and converting to RGB. (c) Difference between the two results.\\n\\n47\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.6 Color image smoothing and sharpening\\n\\n} Color image sharpening\\n\\n} Ex. 6.13 (image sharpening using the Laplacian)\\n\\nQ% M %, & =\\n\\nQ%*(%, &) Q%+(%, &) Q%,(%, &)\\n\\nFigure 6.39 Image sharpening using the Laplacian. (a) Result of processing each RGB channel. (b) Result of processing the HSI intensity component and converting to RGB. (c) Difference between the two results.\\n\\n48\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.7 Using color in image segmentation\\n\\n} Segmentation is a process that partitions an image into regions\\n\\n} Segmentation in HSI color space\\n\\n} If we want to carry out the segmentation process on individual planes, it is natural to think first of the HSI space because color is conveniently represented in the hue image\\n\\n} Saturation is used as a masking image in order to isolate\\n\\nfurther regions of interest in the hue image\\n\\n} The intensity image is used less frequently for segmentation of\\n\\ncolor images because it carries no color information\\n\\n49\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.7 Using color in image segmentation\\n\\n} Segmentation in HSI color space\\n\\n} Ex. 6.14 (segmenting a color image in HSI color space)\\n\\nFigure 6.40 Image segmentation in HSI space. (a) Original. (b) Hue. (c) Saturation. (d) Intensity. (e) Binary saturation mask (black = 0). (f) Product of (b) and (e). (g) Histogram of (f). (h) Segmentation of red components from (a).\\n\\n50\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.7 Using color in image segmentation\\n\\n} Segmentation in RGB space\\n\\n} The objective is to segment objects of a specified color range\\n\\nin an RGB image\\n\\n} Let denote the average color that we wish to segment\\n\\nby a vector 7 } The objective of segmentation is to classify each RGB pixel in a given\\n\\nimage as having a color in the specified range or not\\n\\n} It is necessary to have a measure of similarity } Let @ denote an arbitrary point in RGB space R S, T = S − T = S − T 3(S − T)\\n\\n= !4 − @4\\n\\n% + !5 − @5\\n\\n% + !6 − @6\\n\\n%\\n\\nR S, T = S − T = S − T 3V7)(S − T)\\n\\n51\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.7 Using color in image segmentation\\n\\n} Segmentation in RGB vector space\\n\\n} Three approaches\\n\\nFigure 6.41 Three approaches for enclosing data regions for RGB vector segmentation.\\n\\n} Ex. 6.15 (color segmentation\\n\\nin RGB color space)\\n\\nFigure 6.42 Segmentation in RGB space. (a) Original image with colors of interest shown enclosed by a rectangle. (b) Result of segmentation in RGB vector space. Compare with Fig. 6.40(h).\\n\\n52\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.7 Using color in image segmentation\\n\\n} Color edge detection\\n\\n} The details of edge-based segmentation are given in sec10.2 } Processing the three individual planes to form a composite\\n\\ngradient image can yield erroneous results\\n\\n} If accuracy is an issue, we need a new definition of the gradient\\n\\napplicable to vector quantities\\n\\nFigure 6.43 (a)–(c) R, G, and B component images, and (d) resulting RGB color image. (e)–(g) R, G, and B component images, and (h) resulting RGB color image.\\n\\n53\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.7 Using color in image segmentation\\n\\n} Color edge detection\\n\\n} The direction of maximum rate of change of 8(:, <) is given by\\n\\nthe angle and the value of the rate of change at :, <\\n\\nW =\\n\\n84 89\\n\\nX +\\n\\n85 89\\n\\nY +\\n\\n86 89\\n\\nZ, [ =\\n\\n84 8(\\n\\nX +\\n\\n85 8(\\n\\nY +\\n\\n86 8(\\n\\nZ\\n\\n\\\\99 = W ] W = W3W =\\n\\n%\\n\\n+\\n\\n84 89\\n\\n85 89\\n\\n%\\n\\n+\\n\\n\\\\(( = [ ] [ = [3[ =\\n\\n\\\\9( = W ] [ = W3[ =\\n\\n→ _ %, & =\\n\\ntan7)\\n\\n) %\\n\\n%\\n\\n84\\n\\n+\\n\\n8( 84 89\\n\\n+\\n\\n84 8( %:)\\n\\n\\n\\n:))7:\\n\\n\\n\\n\\n\\n%\\n\\n85\\n\\n8( 85 89\\n\\n85 8(\\n\\n+\\n\\n+\\n\\n,\\n\\n%\\n\\n86 89\\n\\n%\\n\\n86\\n\\n8( 86 89\\n\\n86 8(\\n\\nJB !, ( =\\n\\n1 2\\n\\nHCC + HDD + HCC − HDD cos 2N(!, () + 2HCD sin 2N(!, ()\\n\\n54\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.7 Using color in image segmentation\\n\\n} Color edge detection\\n\\n} Ex. 6.16 (edge detection in RGB color space)\\n\\nFigure 6.44 (a) RGB image. (b) Gradient computed in RGB color vector space. (c) Gradient image formed by the elementwise sum of three individual gradient images, each computed using the Sobel operators. (d) Difference between (b) and (c).\\n\\n55\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.7 Using color in image segmentation\\n\\n} Color edge detection\\n\\n} Ex. 6.16 (edge detection in RGB color space)\\n\\nFigure 6.45 Component gradient images of the color image in Fig. 6.44. (a) Red component, (b) green component, and (c) blue component. These three images were added and scaled to produce the image in Fig. 6.44(c).\\n\\n56\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.8 Noise in color images\\n\\n} The noise content of a color image has the same\\n\\ncharacteristics in each color channel, but it is possible for color channels to be affected differently by noise } Ex. 6.17 (illustration of\\n\\nthe effects when converting noisy RGB images to HSI)\\n\\nFigure 6.46 (a)–(c) Red, green, and blue 8- bit component images corrupted by additive Gaussian noise of mean 0 and standard deviation of 28 intensity levels. (d) Resulting RGB image. [Compare (d) with Fig. 6.44(a).]\\n\\n57\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.8 Noise in color images\\n\\n} Ex. 6.17\\n\\nFigure 6.47 HSI components of the noisy color image in Fig. 6.46(d). (a) Hue. (b) Saturation. (c) Intensity.\\n\\nFigure 6.48 (a) RGB image with green plane corrupted by salt- and-pepper noise. (b) Hue component of HSI image. (c) Saturation component. (d) Intensity component.\\n\\n58\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\nSummary\\n\\n} Color fundamentals } Color models } Pseudocolor image processing } Basics of full-color image processing } Color transformations } Color image smoothing and sharpening } Using color in image segmentation } Noise in color images } Color image compression\\n\\n59\\n\\n6. Color Image Processing\\n\\nSpring 2023', metadata={'source': 'data/dip.pdf'})]\n"
          ]
        }
      ],
      "source": [
        "from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader, DirectoryLoader\n",
        "import glob\n",
        "\n",
        "\n",
        "loader = UnstructuredPDFLoader('data/dip.pdf')\n",
        "docs = loader.load()\n",
        "print(len(docs))\n",
        "print(docs) # pdf uploader\n",
        "# 많은 pdf를 올리면 다음과 같아진다. docs[0] 로 배열을 확인 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "expected string or bytes-like object",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[146], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpinecone\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mprint\u001b[39m(pinecone\u001b[39m.\u001b[39;49mdescribe_index(\u001b[39m'\u001b[39;49m\u001b[39mpdf-text-test\u001b[39;49m\u001b[39m'\u001b[39;49m))\n",
            "File \u001b[0;32m~/miniforge3/envs/langchain_env/lib/python3.8/site-packages/pinecone/manage.py:196\u001b[0m, in \u001b[0;36mdescribe_index\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Describes a Pinecone index.\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \n\u001b[1;32m    192\u001b[0m \u001b[39m:param: the name of the index\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[39m:return: Description of an index\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    195\u001b[0m api_instance \u001b[39m=\u001b[39m _get_api_instance()\n\u001b[0;32m--> 196\u001b[0m response \u001b[39m=\u001b[39m api_instance\u001b[39m.\u001b[39;49mdescribe_index(name)\n\u001b[1;32m    197\u001b[0m db \u001b[39m=\u001b[39m response[\u001b[39m'\u001b[39m\u001b[39mdatabase\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    198\u001b[0m ready \u001b[39m=\u001b[39m response[\u001b[39m'\u001b[39m\u001b[39mstatus\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mready\u001b[39m\u001b[39m'\u001b[39m]\n",
            "File \u001b[0;32m~/miniforge3/envs/langchain_env/lib/python3.8/site-packages/pinecone/core/client/api_client.py:776\u001b[0m, in \u001b[0;36mEndpoint.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    766\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" This method is invoked when endpoints are called\u001b[39;00m\n\u001b[1;32m    767\u001b[0m \u001b[39m    Example:\u001b[39;00m\n\u001b[1;32m    768\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    774\u001b[0m \n\u001b[1;32m    775\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 776\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcallable(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/miniforge3/envs/langchain_env/lib/python3.8/site-packages/pinecone/core/client/api/index_operations_api.py:889\u001b[0m, in \u001b[0;36mIndexOperationsApi.__init__.<locals>.__describe_index\u001b[0;34m(self, index_name, **kwargs)\u001b[0m\n\u001b[1;32m    886\u001b[0m kwargs[\u001b[39m'\u001b[39m\u001b[39m_host_index\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39m_host_index\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    887\u001b[0m kwargs[\u001b[39m'\u001b[39m\u001b[39mindex_name\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \\\n\u001b[1;32m    888\u001b[0m     index_name\n\u001b[0;32m--> 889\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall_with_http_info(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/miniforge3/envs/langchain_env/lib/python3.8/site-packages/pinecone/core/client/api_client.py:838\u001b[0m, in \u001b[0;36mEndpoint.call_with_http_info\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    834\u001b[0m     header_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_client\u001b[39m.\u001b[39mselect_header_content_type(\n\u001b[1;32m    835\u001b[0m         content_type_headers_list)\n\u001b[1;32m    836\u001b[0m     params[\u001b[39m'\u001b[39m\u001b[39mheader\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mContent-Type\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m header_list\n\u001b[0;32m--> 838\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapi_client\u001b[39m.\u001b[39;49mcall_api(\n\u001b[1;32m    839\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msettings[\u001b[39m'\u001b[39;49m\u001b[39mendpoint_path\u001b[39;49m\u001b[39m'\u001b[39;49m], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msettings[\u001b[39m'\u001b[39;49m\u001b[39mhttp_method\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    840\u001b[0m     params[\u001b[39m'\u001b[39;49m\u001b[39mpath\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    841\u001b[0m     params[\u001b[39m'\u001b[39;49m\u001b[39mquery\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    842\u001b[0m     params[\u001b[39m'\u001b[39;49m\u001b[39mheader\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    843\u001b[0m     body\u001b[39m=\u001b[39;49mparams[\u001b[39m'\u001b[39;49m\u001b[39mbody\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    844\u001b[0m     post_params\u001b[39m=\u001b[39;49mparams[\u001b[39m'\u001b[39;49m\u001b[39mform\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    845\u001b[0m     files\u001b[39m=\u001b[39;49mparams[\u001b[39m'\u001b[39;49m\u001b[39mfile\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    846\u001b[0m     response_type\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msettings[\u001b[39m'\u001b[39;49m\u001b[39mresponse_type\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    847\u001b[0m     auth_settings\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msettings[\u001b[39m'\u001b[39;49m\u001b[39mauth\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    848\u001b[0m     async_req\u001b[39m=\u001b[39;49mkwargs[\u001b[39m'\u001b[39;49m\u001b[39masync_req\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    849\u001b[0m     _check_type\u001b[39m=\u001b[39;49mkwargs[\u001b[39m'\u001b[39;49m\u001b[39m_check_return_type\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    850\u001b[0m     _return_http_data_only\u001b[39m=\u001b[39;49mkwargs[\u001b[39m'\u001b[39;49m\u001b[39m_return_http_data_only\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    851\u001b[0m     _preload_content\u001b[39m=\u001b[39;49mkwargs[\u001b[39m'\u001b[39;49m\u001b[39m_preload_content\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    852\u001b[0m     _request_timeout\u001b[39m=\u001b[39;49mkwargs[\u001b[39m'\u001b[39;49m\u001b[39m_request_timeout\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    853\u001b[0m     _host\u001b[39m=\u001b[39;49m_host,\n\u001b[1;32m    854\u001b[0m     collection_formats\u001b[39m=\u001b[39;49mparams[\u001b[39m'\u001b[39;49m\u001b[39mcollection_format\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
            "File \u001b[0;32m~/miniforge3/envs/langchain_env/lib/python3.8/site-packages/pinecone/core/client/api_client.py:413\u001b[0m, in \u001b[0;36mApiClient.call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, async_req, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Makes the HTTP request (synchronous) and returns deserialized data.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[39mTo make an async_req request, set the async_req parameter.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[39m    then the method will return the response directly.\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    412\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m async_req:\n\u001b[0;32m--> 413\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__call_api(resource_path, method,\n\u001b[1;32m    414\u001b[0m                            path_params, query_params, header_params,\n\u001b[1;32m    415\u001b[0m                            body, post_params, files,\n\u001b[1;32m    416\u001b[0m                            response_type, auth_settings,\n\u001b[1;32m    417\u001b[0m                            _return_http_data_only, collection_formats,\n\u001b[1;32m    418\u001b[0m                            _preload_content, _request_timeout, _host,\n\u001b[1;32m    419\u001b[0m                            _check_type)\n\u001b[1;32m    421\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool\u001b[39m.\u001b[39mapply_async(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__call_api, (resource_path,\n\u001b[1;32m    422\u001b[0m                                                method, path_params,\n\u001b[1;32m    423\u001b[0m                                                query_params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    431\u001b[0m                                                _request_timeout,\n\u001b[1;32m    432\u001b[0m                                                _host, _check_type))\n",
            "File \u001b[0;32m~/miniforge3/envs/langchain_env/lib/python3.8/site-packages/pinecone/core/client/api_client.py:200\u001b[0m, in \u001b[0;36mApiClient.__call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[0m\n\u001b[1;32m    196\u001b[0m     url \u001b[39m=\u001b[39m _host \u001b[39m+\u001b[39m resource_path\n\u001b[1;32m    198\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m     \u001b[39m# perform request and return response\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m     response_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    201\u001b[0m         method, url, query_params\u001b[39m=\u001b[39;49mquery_params, headers\u001b[39m=\u001b[39;49mheader_params,\n\u001b[1;32m    202\u001b[0m         post_params\u001b[39m=\u001b[39;49mpost_params, body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    203\u001b[0m         _preload_content\u001b[39m=\u001b[39;49m_preload_content,\n\u001b[1;32m    204\u001b[0m         _request_timeout\u001b[39m=\u001b[39;49m_request_timeout)\n\u001b[1;32m    205\u001b[0m \u001b[39mexcept\u001b[39;00m ApiException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    206\u001b[0m     e\u001b[39m.\u001b[39mbody \u001b[39m=\u001b[39m e\u001b[39m.\u001b[39mbody\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m)\n",
            "File \u001b[0;32m~/miniforge3/envs/langchain_env/lib/python3.8/site-packages/pinecone/core/client/api_client.py:439\u001b[0m, in \u001b[0;36mApiClient.request\u001b[0;34m(self, method, url, query_params, headers, post_params, body, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Makes the HTTP request using RESTClient.\"\"\"\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[39mif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mGET\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 439\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrest_client\u001b[39m.\u001b[39;49mGET(url,\n\u001b[1;32m    440\u001b[0m                                 query_params\u001b[39m=\u001b[39;49mquery_params,\n\u001b[1;32m    441\u001b[0m                                 _preload_content\u001b[39m=\u001b[39;49m_preload_content,\n\u001b[1;32m    442\u001b[0m                                 _request_timeout\u001b[39m=\u001b[39;49m_request_timeout,\n\u001b[1;32m    443\u001b[0m                                 headers\u001b[39m=\u001b[39;49mheaders)\n\u001b[1;32m    444\u001b[0m \u001b[39melif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mHEAD\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    445\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrest_client\u001b[39m.\u001b[39mHEAD(url,\n\u001b[1;32m    446\u001b[0m                                  query_params\u001b[39m=\u001b[39mquery_params,\n\u001b[1;32m    447\u001b[0m                                  _preload_content\u001b[39m=\u001b[39m_preload_content,\n\u001b[1;32m    448\u001b[0m                                  _request_timeout\u001b[39m=\u001b[39m_request_timeout,\n\u001b[1;32m    449\u001b[0m                                  headers\u001b[39m=\u001b[39mheaders)\n",
            "File \u001b[0;32m~/miniforge3/envs/langchain_env/lib/python3.8/site-packages/pinecone/core/client/rest.py:236\u001b[0m, in \u001b[0;36mRESTClientObject.GET\u001b[0;34m(self, url, headers, query_params, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mGET\u001b[39m(\u001b[39mself\u001b[39m, url, headers\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, query_params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, _preload_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    235\u001b[0m         _request_timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 236\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(\u001b[39m\"\u001b[39;49m\u001b[39mGET\u001b[39;49m\u001b[39m\"\u001b[39;49m, url,\n\u001b[1;32m    237\u001b[0m                         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    238\u001b[0m                         _preload_content\u001b[39m=\u001b[39;49m_preload_content,\n\u001b[1;32m    239\u001b[0m                         _request_timeout\u001b[39m=\u001b[39;49m_request_timeout,\n\u001b[1;32m    240\u001b[0m                         query_params\u001b[39m=\u001b[39;49mquery_params)\n",
            "File \u001b[0;32m~/miniforge3/envs/langchain_env/lib/python3.8/site-packages/pinecone/core/client/rest.py:202\u001b[0m, in \u001b[0;36mRESTClientObject.request\u001b[0;34m(self, method, url, query_params, headers, body, post_params, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[39mraise\u001b[39;00m ApiException(status\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, reason\u001b[39m=\u001b[39mmsg)\n\u001b[1;32m    200\u001b[0m     \u001b[39m# For `GET`, `HEAD`\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 202\u001b[0m         r \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpool_manager\u001b[39m.\u001b[39;49mrequest(method, url,\n\u001b[1;32m    203\u001b[0m                                       fields\u001b[39m=\u001b[39;49mquery_params,\n\u001b[1;32m    204\u001b[0m                                       preload_content\u001b[39m=\u001b[39;49m_preload_content,\n\u001b[1;32m    205\u001b[0m                                       timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    206\u001b[0m                                       headers\u001b[39m=\u001b[39;49mheaders)\n\u001b[1;32m    207\u001b[0m \u001b[39mexcept\u001b[39;00m urllib3\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mSSLError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    208\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mtype\u001b[39m(e)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39mstr\u001b[39m(e))\n",
            "File \u001b[0;32m~/miniforge3/envs/langchain_env/lib/python3.8/site-packages/urllib3/_request_methods.py:110\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[0;34m(self, method, url, body, fields, headers, json, **urlopen_kw)\u001b[0m\n\u001b[1;32m    107\u001b[0m     urlopen_kw[\u001b[39m\"\u001b[39m\u001b[39mbody\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m body\n\u001b[1;32m    109\u001b[0m \u001b[39mif\u001b[39;00m method \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_encode_url_methods:\n\u001b[0;32m--> 110\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest_encode_url(\n\u001b[1;32m    111\u001b[0m         method,\n\u001b[1;32m    112\u001b[0m         url,\n\u001b[1;32m    113\u001b[0m         fields\u001b[39m=\u001b[39;49mfields,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    114\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    115\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49murlopen_kw,\n\u001b[1;32m    116\u001b[0m     )\n\u001b[1;32m    117\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_encode_body(\n\u001b[1;32m    119\u001b[0m         method, url, fields\u001b[39m=\u001b[39mfields, headers\u001b[39m=\u001b[39mheaders, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39murlopen_kw\n\u001b[1;32m    120\u001b[0m     )\n",
            "File \u001b[0;32m~/miniforge3/envs/langchain_env/lib/python3.8/site-packages/urllib3/_request_methods.py:143\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_url\u001b[0;34m(self, method, url, fields, headers, **urlopen_kw)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39mif\u001b[39;00m fields:\n\u001b[1;32m    141\u001b[0m     url \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m?\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m urlencode(fields)\n\u001b[0;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murlopen(method, url, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mextra_kw)\n",
            "File \u001b[0;32m~/miniforge3/envs/langchain_env/lib/python3.8/site-packages/urllib3/poolmanager.py:433\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[0;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[1;32m    431\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39murlopen(method, url, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[1;32m    432\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 433\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(method, u\u001b[39m.\u001b[39;49mrequest_uri, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    435\u001b[0m redirect_location \u001b[39m=\u001b[39m redirect \u001b[39mand\u001b[39;00m response\u001b[39m.\u001b[39mget_redirect_location()\n\u001b[1;32m    436\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m redirect_location:\n",
            "File \u001b[0;32m~/miniforge3/envs/langchain_env/lib/python3.8/site-packages/urllib3/connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    787\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    789\u001b[0m \u001b[39m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    791\u001b[0m     conn,\n\u001b[1;32m    792\u001b[0m     method,\n\u001b[1;32m    793\u001b[0m     url,\n\u001b[1;32m    794\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    795\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    796\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    797\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    798\u001b[0m     retries\u001b[39m=\u001b[39;49mretries,\n\u001b[1;32m    799\u001b[0m     response_conn\u001b[39m=\u001b[39;49mresponse_conn,\n\u001b[1;32m    800\u001b[0m     preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    801\u001b[0m     decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    802\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw,\n\u001b[1;32m    803\u001b[0m )\n\u001b[1;32m    805\u001b[0m \u001b[39m# Everything went great!\u001b[39;00m\n\u001b[1;32m    806\u001b[0m clean_exit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
            "File \u001b[0;32m~/miniforge3/envs/langchain_env/lib/python3.8/site-packages/urllib3/connectionpool.py:496\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[39m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[1;32m    494\u001b[0m \u001b[39m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n\u001b[1;32m    495\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 496\u001b[0m     conn\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    497\u001b[0m         method,\n\u001b[1;32m    498\u001b[0m         url,\n\u001b[1;32m    499\u001b[0m         body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    500\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    501\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    502\u001b[0m         preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    503\u001b[0m         decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    504\u001b[0m         enforce_content_length\u001b[39m=\u001b[39;49menforce_content_length,\n\u001b[1;32m    505\u001b[0m     )\n\u001b[1;32m    507\u001b[0m \u001b[39m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[39m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[1;32m    509\u001b[0m \u001b[39m# With this behaviour, the received response is still readable.\u001b[39;00m\n\u001b[1;32m    510\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBrokenPipeError\u001b[39;00m:\n",
            "File \u001b[0;32m~/miniforge3/envs/langchain_env/lib/python3.8/site-packages/urllib3/connection.py:387\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mputheader(\u001b[39m\"\u001b[39m\u001b[39mUser-Agent\u001b[39m\u001b[39m\"\u001b[39m, _get_default_user_agent())\n\u001b[1;32m    386\u001b[0m \u001b[39mfor\u001b[39;00m header, value \u001b[39min\u001b[39;00m headers\u001b[39m.\u001b[39mitems():\n\u001b[0;32m--> 387\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mputheader(header, value)\n\u001b[1;32m    388\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mendheaders()\n\u001b[1;32m    390\u001b[0m \u001b[39m# If we're given a body we start sending that in chunks.\u001b[39;00m\n",
            "File \u001b[0;32m~/miniforge3/envs/langchain_env/lib/python3.8/site-packages/urllib3/connection.py:301\u001b[0m, in \u001b[0;36mHTTPConnection.putheader\u001b[0;34m(self, header, *values)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\"\"\"\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39many\u001b[39m(\u001b[39misinstance\u001b[39m(v, \u001b[39mstr\u001b[39m) \u001b[39mand\u001b[39;00m v \u001b[39m==\u001b[39m SKIP_HEADER \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m values):\n\u001b[0;32m--> 301\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mputheader(header, \u001b[39m*\u001b[39;49mvalues)\n\u001b[1;32m    302\u001b[0m \u001b[39melif\u001b[39;00m to_str(header\u001b[39m.\u001b[39mlower()) \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m SKIPPABLE_HEADERS:\n\u001b[1;32m    303\u001b[0m     skippable_headers \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\n\u001b[1;32m    304\u001b[0m         [\u001b[39mstr\u001b[39m\u001b[39m.\u001b[39mtitle(header) \u001b[39mfor\u001b[39;00m header \u001b[39min\u001b[39;00m \u001b[39msorted\u001b[39m(SKIPPABLE_HEADERS)]\n\u001b[1;32m    305\u001b[0m     )\n",
            "File \u001b[0;32m~/miniforge3/envs/langchain_env/lib/python3.8/http/client.py:1233\u001b[0m, in \u001b[0;36mHTTPConnection.putheader\u001b[0;34m(self, header, *values)\u001b[0m\n\u001b[1;32m   1230\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(one_value, \u001b[39mint\u001b[39m):\n\u001b[1;32m   1231\u001b[0m         values[i] \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(one_value)\u001b[39m.\u001b[39mencode(\u001b[39m'\u001b[39m\u001b[39mascii\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 1233\u001b[0m     \u001b[39mif\u001b[39;00m _is_illegal_header_value(values[i]):\n\u001b[1;32m   1234\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mInvalid header value \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (values[i],))\n\u001b[1;32m   1236\u001b[0m value \u001b[39m=\u001b[39m \u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(values)\n",
            "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
          ]
        }
      ],
      "source": [
        "import pinecone\n",
        "print(pinecone.describe_index('pdf-text-test'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVAcsY7JtXPn"
      },
      "source": [
        "We access the plaintext page content like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "JPljMKcytXPn",
        "outputId": "47087370-e65f-40f4-d8b9-9df30a897c1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It is only after years of preparation that the young artist should touch color– not color used descriptively, that is, but as a means of personal expression.\n",
            "\n",
            "Henri Matisse\n",
            "\n",
            "For a long time I limited myself to one color – as a form of discipline.\n",
            "\n",
            "Pablo Picasso\n",
            "\n",
            "Color Image Processing\n",
            "\n",
            "Chapter 6 어\n",
            "\n",
            "Preview } Motivation of using colors in image processing\n",
            "\n",
            "} A powerful descriptor that simplifies object identification } Thousands of color shades and intensities can be discerned by\n",
            "\n",
            "humans\n",
            "\n",
            "} Two major areas in color image processing\n",
            "\n",
            "} Full\n",
            "\n",
            "\n",
            "\n",
            "color processing\n",
            "\n",
            "} images are acquired with a full-color sensor\n",
            "\n",
            "} Pseudo\n",
            "\n",
            "\n",
            "\n",
            "color processing\n",
            "\n",
            "} assigning a color to a particular monochrome intensity or\n",
            "\n",
            "range of intensities\n",
            "\n",
            "} Full-color image processing techniques are used in broad\n",
            "\n",
            "range of applications, as the hardware cost is getting reasonable\n",
            "\n",
            "2\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Spring 2023\n",
            "\n",
            "Preview\n",
            "\n",
            "} This chapter covers } Color fundamentals } Color models } Pseudocolor image processing } Basics of full-color image processing } Color transformations } Color image smoothing and sharpening } Using color in image segmentation } Noise in color images } Color image compression\n",
            "\n",
            "3\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Spring 2023\n",
            "\n",
            "6.1 Color fundamentals\n",
            "\n",
            "} The process followed by the human brain in perceiving\n",
            "\n",
            "and interpreting color is a physiopsychological phenomenon that is not yet fully understood\n",
            "\n",
            "} Sir Issac Newton – color spectrum } The colors that humans and some other animals perceive in an object are determined by the nature of the light reflected from the object\n",
            "\n",
            "Figure 6.1 Color spectrum seen by passing white light through a prism. (Courtesy of the General Electric Co., Lighting Division.)\n",
            "\n",
            "4\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Spring 2023\n",
            "\n",
            "6.1 Color fundamentals } Characterization of light is central to the science of color\n",
            "\n",
            "} Achromatic: the only attribute is its intensity\n",
            "\n",
            "} Chromatic light: the electromagnetic spectrum\n",
            "\n",
            "from approximately 400 – 700 %&\n",
            "\n",
            "Figure 6.2 Wavelengths comprising the visible range of the electromagnetic spectrum. (Courtesy of the General Electric Co., Lighting Division.)\n",
            "\n",
            "} Three basic quantities used to describe the quality of a chromatic light source: radiance, luminance, and brightness\n",
            "\n",
            "5\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Spring 2023\n",
            "\n",
            "6.1 Color fundamentals } Radiance\n",
            "\n",
            "} The total amount of energy that flows from the light source } It is measured in watts (!)\n",
            "\n",
            "} Luminance\n",
            "\n",
            "} A measure of the amount of energy an observer perceives\n",
            "\n",
            "from a light source\n",
            "\n",
            "} It is measured in lumens (\"#)\n",
            "\n",
            "} Brightness\n",
            "\n",
            "} A subjective descriptor that is practically impossible to\n",
            "\n",
            "measure\n",
            "\n",
            "} It embodies the achromatic notion of intensity and\n",
            "\n",
            "is one of the key factors in describing color sensation\n",
            "\n",
            "6\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Spring 2023\n",
            "\n",
            "6.1 Color fundamentals\n",
            "\n",
            "} Due to the absorption characteristics of the human eye, colors are seen as variable combinations of the primary colors } red (700 &#), green (546.1 &#), and blue (435.8 &#)\n",
            "\n",
            "Figure 6.3 Absorption of light by the red, green, and blue cones in the human eye as a function of wavelength.\n",
            "\n",
            "7\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Spring 2023\n",
            "\n",
            "6.1 Color fundamentals\n",
            "\n",
            "} The secondary colors of light\n",
            "\n",
            "} magenta (. + 0), cyan (1 + 0), yellow (. + 1) } Differentiating between the primary colors of light and the\n",
            "\n",
            "primary colors of pigments or colorants is important\n",
            "\n",
            "Figure 6.4 Primary and secondary colors of light and pigments. (Courtesy of the General Electric Co., Lighting Division.)\n",
            "\n",
            "8\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Spring 2023\n",
            "\n",
            "6.1 Color fundamentals } Color TV reception is an example of the additive nature\n",
            "\n",
            "of light colors\n",
            "\n",
            "} The characteristics used to distinguish one color from\n",
            "\n",
            "another are brightness, hue, and saturation } Hue\n",
            "\n",
            "} An attribute associated with the dominant wavelength in a mixture of light waves; it represents dominant color as perceived by an observer\n",
            "\n",
            "} When we call an object red, orange, or yellow, we are specifying its\n",
            "\n",
            "hue } Saturation\n",
            "\n",
            "} The relative purity or the amount of white light mixed with a hue } The pure spectrum colors are fully saturated } The degree of saturation is inversely proportional to the amount of\n",
            "\n",
            "white light added\n",
            "\n",
            "} Chromaticity: hue and saturation\n",
            "\n",
            "9\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Spring 2023\n",
            "\n",
            "6.1 Color fundamentals\n",
            "\n",
            "} Tristimulus values\n",
            "\n",
            "} The amounts of red (2), green (3), and blue (4) needed to\n",
            "\n",
            "form any particular color\n",
            "\n",
            "} A color is specified by its trichromatic coefficients (F6.5)\n",
            "\n",
            "! =\n",
            "\n",
            "# # + % + &\n",
            "\n",
            ",\n",
            "\n",
            "( =\n",
            "\n",
            "% # + % + &\n",
            "\n",
            ",\n",
            "\n",
            ") =\n",
            "\n",
            "& # + % + &\n",
            "\n",
            "→ ! + ( + ) = 1\n",
            "\n",
            "} Another approach for specifying colors\n",
            "\n",
            "– CIE chromaticity diagram } It shows color composition as a function of ! (red) and \" (green) } The corresponding value of # (blue) is obtained from the equation\n",
            "\n",
            "! = 1 − % − &\n",
            "\n",
            "10\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Spring 2023\n",
            "\n",
            "6.1 Color fundamentals\n",
            "\n",
            "} The chromaticity diagram:\n",
            "\n",
            "} useful for color mixing because a straight-line segment joining any two\n",
            "\n",
            "points in the diagram defines all the different color variations that can be obtained by combining these two colors additively\n",
            "\n",
            "} A typical range of colors (the color gamut) by RGB monitors and\n",
            "\n",
            "printing devices\n",
            "\n",
            "Figure 6.5 The C I E chromaticity diagram. (Courtesy of the General Electric Co., Lighting Division.)\n",
            "\n",
            "Figure 6.6 Illustrative color gamut of color monitors (triangle) and color printing devices (shaded region).\n",
            "\n",
            "11\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Spring 2023\n",
            "\n",
            "6.1 Color fundamentals } 3-CCD/CMOS camera\n",
            "\n",
            "https://mvpromedia.com/wp\n",
            "\n",
            "\n",
            "\n",
            "content/uploads/2020/02/JAI\n",
            "\n",
            "\n",
            "\n",
            "Press\n",
            "\n",
            "\n",
            "\n",
            "Photo_Fusion\n",
            "\n",
            "\n",
            "\n",
            "Series_FS\n",
            "\n",
            "\n",
            "\n",
            "3200T\n",
            "\n",
            "\n",
            "\n",
            "10GE\n",
            "\n",
            "\n",
            "\n",
            "Prism\n",
            "\n",
            "\n",
            "\n",
            "Illus tration_2\n",
            "\n",
            "\n",
            "\n",
            "JPEG\n",
            "\n",
            "\n",
            "\n",
            "format\n",
            "\n",
            "\n",
            "\n",
            "scaled.jpg\n",
            "\n",
            "12\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Spring 2023\n",
            "\n",
            "6.1 Color fundamentals } Bayer Pattern\n",
            "\n",
            "} Color interpolation\n",
            "\n",
            "13\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Spring 2023\n",
            "\n",
            "https://www.1stvision.com/machine\n",
            "\n",
            "\n",
            "\n",
            "vision\n",
            "\n",
            "\n",
            "\n",
            "solutions/wp\n",
            "\n",
            "\n",
            "\n",
            "content/uploads/2018/04/b ayer\n",
            "\n",
            "\n",
            "\n",
            "vs\n",
            "\n",
            "\n",
            "\n",
            "3cmos\n",
            "\n",
            "\n",
            "\n",
            "prism.png\n",
            "\n",
            "6.2 Color models\n",
            "\n",
            "} The purpose of a color model\n",
            "\n",
            "} To facilitate the specification of colors in some standard way } A specification of a coordinate system and a subspace within that system where each color is represented by a single point\n",
            "\n",
            "} RGB: for color monitors and a broad class of color video\n",
            "\n",
            "cameras\n",
            "\n",
            "} CMY (cyan, magenta, yellow) and CMYK (+black) for color\n",
            "\n",
            "printing\n",
            "\n",
            "} HSI (hue, saturation, intensity)\n",
            "\n",
            "} corresponds to the way humans describe and interpret color } it decouples the color and gray-scale information in an image, making it suitable for many of the gray-scale techniques developed in this book\n",
            "\n",
            "} Numerous color models\n",
            "\n",
            "14\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Spring 2023\n",
            "\n",
            "6.2 Color models\n",
            "\n",
            "} The RGB color model\n",
            "\n",
            "} Based on a Cartesian coordinate system } The different colors are points on or inside the cube, and are\n",
            "\n",
            "defined by vector extending from the origin\n",
            "\n",
            "} Pixel depth: the number of bits used to represent each pixel\n",
            "\n",
            "in RGB space } A depth of 24 bits: an RGB image in which each of the red, green, and\n",
            "\n",
            "blue images is an 8-bit image (full-color: 16,777,216)\n",
            "\n",
            "Figure 6.7 Schematic of the RGB color cube. Points along the main diagonal have gray values, from black at the origin to white at point (1, 1, 1).\n",
            "\n",
            "Figure 6.8 A 24\n",
            "\n",
            "\n",
            "\n",
            "bit RGB color cube.\n",
            "\n",
            "15\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Spring 2023\n",
            "\n",
            "6.2 Color models\n",
            "\n",
            "} The RGB color model\n",
            "\n",
            "} Ex. 6.1 (generating a cross-section of the RGB color cube and\n",
            "\n",
            "its three hidden planes)\n",
            "\n",
            "Figure 6.9 (a) Generating the RGB image of the cross-sectional color plane (127, G, B). (b) The three hidden surface planes in the color cube of Fig. 6.8.\n",
            "\n",
            "16\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Spring 2023\n",
            "\n",
            "6.2 Color models\n",
            "\n",
            "} The CMY and CMYK color models\n",
            "\n",
            "} Most devices that deposit colored pigments on paper, such as\n",
            "\n",
            "color printers and copiers, require CMY data input or perform an RGB to CMY conversion internally\n",
            "\n",
            "' ( )\n",
            "\n",
            "=\n",
            "\n",
            "1 1 1\n",
            "\n",
            "−\n",
            "\n",
            "+ ,\n",
            "\n",
            "} Equal amount of the pigment primaries, 5, 6, and 3 should\n",
            "\n",
            "produce black } Combining these colors for printing produces a muddy-looking black } In order to produce true black, a fourth color, black, is added, giving\n",
            "\n",
            "rise to the CMYK color model\n",
            "\n",
            "17\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Spring 2023\n",
            "\n",
            "6.2 Color models\n",
            "\n",
            "} The CMY and CMYK color models\n",
            "\n",
            "} Pixelwise operation\n",
            "\n",
            "} Conversion from CMY to CMYK\n",
            "\n",
            "} $ = min(*, ,, -) } If $ = 1 pure black , * = , = - = 0 Otherwise 1 − $ * = * − $, * =\n",
            "\n",
            "1 − $ , = , − $, , =\n",
            "\n",
            "!\"# $\"# %\"# $\"#\n",
            "\n",
            "1 − $\n",
            "\n",
            "\n",
            "\n",
            "=\n",
            "\n",
            "\n",
            "\n",
            "− $,\n",
            "\n",
            "=\n",
            "\n",
            "&\"# $\"#\n",
            "\n",
            "¨ In the rage [0, 1]\n",
            "\n",
            "} Conversion from CMYK to CMY\n",
            "\n",
            "= * 1 − $ + $ , = , 1 − $ + $ - = - 1 − $ + $\n",
            "\n",
            "18\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Spring 2023\n",
            "\n",
            "6.2 Color models\n",
            "\n",
            "} The HSI color model\n",
            "\n",
            "} The model decouples the intensity component from the color- carrying information (hue and saturation) in a color image } The model is an ideal tool for developing image processing algorithms based on color descriptions that are natural and intuitive to humans\n",
            "\n",
            "} Extracting intensity from an RGB image\n",
            "\n",
            "Figure 6.10 Conceptual relationships between the RGB and HSI color models.\n",
            "\n",
            "19\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Spring 2023\n",
            "\n",
            "6.2 Color models\n",
            "\n",
            "} The HSI color model\n",
            "\n",
            "Figure 6.11 Hue and saturation in the HSI color model. The dot is any color point. The angle from the red axis gives the hue. The length of the vector is the saturation. The intensity of all colors in any of these planes is given by the position of the plane on the vertical intensity axis.\n",
            "\n",
            "20\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Spring 2023\n",
            "\n",
            "Figure 6.12 The HSI color model based on (a) triangular, and (b) circular color planes. The triangles and circles are perpendicular to the vertical intensity axis.\n",
            "\n",
            "6.2 Color models } The HSI color model\n",
            "\n",
            "} Converting colors from RGB to HSI\n",
            "\n",
            "} ! = #\n",
            "\n",
            "$, 360 − $,\n",
            "\n",
            "if ( ≤\n",
            "\n",
            "\n",
            "\n",
            "if ( >\n",
            "\n",
            "\n",
            "\n",
            "0ℎ232 $ = cos!\"\n",
            "\n",
            "[ $!% &($!()]\n",
            "\n",
            "! \" $!% \"&($!()(%!()\n",
            "\n",
            "} 7 = 1 −\n",
            "\n",
            "+\n",
            "\n",
            "$&%&(\n",
            "\n",
            "min ;,\n",
            "\n",
            "\n",
            "\n",
            ", (\n",
            "\n",
            "} < =\n",
            "\n",
            "\"\n",
            "\n",
            "+\n",
            "\n",
            "(; +\n",
            "\n",
            "\n",
            "\n",
            "+ ()\n",
            "\n",
            "} Converting colors from HSI to RGB\n",
            "\n",
            "} There are three sectors of interest, corresponding to the 120° intervals in the\n",
            "\n",
            "separation of primaries\n",
            "\n",
            "} RG sector (0∘ ≤ ! ≤ 120∘)\n",
            "\n",
            "¨ ' = ) 1 − , , . = ) 1 +\n",
            "\n",
            "! \"#$ % \"#$ &'°(%\n",
            "\n",
            ", 0 = 3) − (. + ')\n",
            "\n",
            "} GB sector (120∘ ≤ ! ≤ 240∘) ! = ! − 120°\n",
            "\n",
            "¨ . = ) 1 − , , 0 = ) 1 +\n",
            "\n",
            "! \"#$ % \"#$ &'°(%\n",
            "\n",
            ", ' = 3) − (. + 0)\n",
            "\n",
            "} BR sector (240∘ ≤ ! ≤ 360∘) ! = ! − 240°\n",
            "\n",
            "¨ 0 = ) 1 − , , ' = ) 1 +\n",
            "\n",
            "! \"#$ % \"#$ &'°(%\n",
            "\n",
            ", . = 3) − (0 + ')\n",
            "\n",
            "21\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Spring 2023\n",
            "\n",
            "6.2 Color models\n",
            "\n",
            "} The HSI color model\n",
            "\n",
            "} Converting colors from HSI to RGB\n",
            "\n",
            "} Ex. 6.2 (the HSI values corresponding to the image of the RGB color\n",
            "\n",
            "cube)\n",
            "\n",
            "Figure 6.13 HSI components of the image in Fig. 6.8: (a) hue, (b) saturation, and (c) intensity images.\n",
            "\n",
            "22\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Spring 2023\n",
            "\n",
            "6.2 Color models\n",
            "\n",
            "} Manipulating HSI component images\n",
            "\n",
            "Figure 6.14 (a) RGB image and the components of its corresponding HSI image: (b) hue, (c) saturation, and (d) intensity.\n",
            "\n",
            "Figure 6.15 (a)-(c) Modified HSI component images. (d) Resulting RGB image. (See Fig. 6.14 for the original HSI images.)\n",
            "\n",
            "23\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Spring 2023\n",
            "\n",
            "6.2 Color models } A device independent color model\n",
            "\n",
            "} In conjunction with digital cameras, flatbed scanner, and inkjet\n",
            "\n",
            "printer, a PC can be turned into a digital darkroom\n",
            "\n",
            "} A device independent color model\n",
            "\n",
            "} Relates the color gamuts of the monitors and output devices } CIELAB\n",
            "\n",
            ",∗ = 116 . ℎ\n",
            "\n",
            "1∗ = 500 ℎ\n",
            "\n",
            "4∗ = 200 ℎ\n",
            "\n",
            "% %/ # #/ 0 0#\n",
            "\n",
            "− 16\n",
            "\n",
            "− ℎ\n",
            "\n",
            "− ℎ\n",
            "\n",
            "% %/ 1 1#\n",
            "\n",
            ",\n",
            "\n",
            "6ℎ787 ℎ(:) = <\n",
            "\n",
            "$ :,\n",
            "\n",
            ": > 0.008856 7.787: + 16/116, : ≤ 0.008856\n",
            "\n",
            "} While not a directly displayable format, its gamut encompasses the\n",
            "\n",
            "entire visible spectrum and can represent accurately the colors of any device\n",
            "\n",
            "24\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Spring 2023\n",
            "\n",
            "6.3 Pseudocolor image processing\n",
            "\n",
            "} Pseudocolor (false color)\n",
            "\n",
            "image processing consists of assigning colors to gray values based on a specified criterion\n",
            "\n",
            "} For human visualization and interpretation of gray-scale events in an\n",
            "\n",
            "image or sequence of image\n",
            "\n",
            "} Intensity slicing and color coding\n",
            "\n",
            "} >(!, \") = ?4 to slice the image function into two levels (Fig. 6.16) if 3 %, & ∈ 5! 3 %, & = 6!\n",
            "\n",
            "} Alternate representation (Fig. 6.17)\n",
            "\n",
            "Figure 6.16 Graphical interpretation of the intensity slicing technique.\n",
            "\n",
            "Figure 6.17 An alternative representation of the intensity slicing technique.\n",
            "\n",
            "25\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Spring 2023\n",
            "\n",
            "6.3 Pseudocolor image processing\n",
            "\n",
            "} Intensity slicing and color coding\n",
            "\n",
            "} Ex. 6.3 (intensity slicing and color coding)\n",
            "\n",
            "} Regions that appear of constant intensity in the monochrome image are quite variable. The color image shows eight different regions of constant intensity\n",
            "\n",
            "Figure 6.18 (a) Grayscale image of the Picker Thyroid Phantom. (b) Result of intensity slicing using eight colors. (Courtesy of Dr. J. L. Blankenship, Oak Ridge National Laboratory.)\n",
            "\n",
            "26\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Spring 2023\n",
            "\n",
            "6.3 Pseudocolor image processing\n",
            "\n",
            "} Intensity slicing and color coding\n",
            "\n",
            "} Subdivision of gray scale based on physical characteristics\n",
            "\n",
            "of the image\n",
            "\n",
            "Figure 6.19 (a) X-ray image of a weld. (b) Result of color coding. (Original image courtesy of X-T E K Systems, Ltd.)\n",
            "\n",
            "27\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Spring 2023\n",
            "\n",
            "6.3 Pseudocolor image processing\n",
            "\n",
            "} Intensity slicing and color coding\n",
            "\n",
            "} Ex. 6.4 (use of color to highlight rainfall levels)\n",
            "\n",
            "Figure 6.20 (a) Grayscale image in which intensity (in the horizontal band shown) corresponds to average monthly rainfall. (b) Colors assigned to intensity values. (c) Color-coded image. (d) Zoom of the South American region. (Courtesy of NASA.)\n",
            "\n",
            "28\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Spring 2023\n",
            "\n",
            "6.3 Pseudocolor image processing\n",
            "\n",
            "} Intensity to color transformations\n",
            "\n",
            "} Three independent transformations on the gray level\n",
            "\n",
            "of any input pixel } The method discussed in the previous section is\n",
            "\n",
            "a special case of the technique\n",
            "\n",
            "Figure 6.21 Functional block diagram for pseudocolor image processing. Images 2\", 2#, and 2$ are fed into the corresponding red, green, and blue inputs of an RGB color monitor.\n",
            "\n",
            "29\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Spring 2023\n",
            "\n",
            "6.3 Pseudocolor image processing\n",
            "\n",
            "} Intensity to color transformations\n",
            "\n",
            "} Ex. 6.5 (using pseudocolor to highlight explosives in X-ray\n",
            "\n",
            "images) : an airport X-ray scanning system\n",
            "\n",
            "Figure 6.22 Pseudocolor enhancement by using the gray level to color transformations in Fig. 7.23. (Original image courtesy of Dr. Mike Hurwitz, Westinghouse.)\n",
            "\n",
            "Figure 6.23 Transformation functions used to obtain the pseudocolor images in Fig. 6.22.\n",
            "\n",
            "30\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Spring 2023\n",
            "\n",
            "6.3 Pseudocolor image processing\n",
            "\n",
            "} Intensity to color transformations\n",
            "\n",
            "} Multispectral image processing\n",
            "\n",
            "} Different sensors produce individual monochrome images,\n",
            "\n",
            "each in a different spectral band\n",
            "\n",
            "} Additional processing\n",
            "\n",
            "} color balancing, combining images, and selecting the three images for\n",
            "\n",
            "display\n",
            "\n",
            "Figure 6.24 A pseudocolor coding approach using multiple grayscale images. The inputs are grayscale images. The outputs are the three components of an RGB composite image.\n",
            "\n",
            "31\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Spring 2023\n",
            "\n",
            "6.3 Pseudocolor image processing\n",
            "\n",
            "} Intensity to color transformations\n",
            "\n",
            "} Ex. 6.6 (color coding of multispectral images)\n",
            "\n",
            "Figure 6.25 (a)–(d) Red (R), green (G), blue (B), and near-infrared (IR) components of a LANDSAT multispectral image of the Washington, D.C. area. (e) RGB color composite image obtained using the IR, G, and B component images. (f) RGB color composite image obtained using the R, IR, and B component images. (Original multispectral images courtesy of NASA.)\n",
            "\n",
            "32\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Figure 6.26 (a) Pseudocolor rendition of Jupiter Moon Io. (b) A close-up. (Courtesy of NASA.)\n",
            "\n",
            "Spring 2023\n",
            "\n",
            "6.4 Basics of full\n",
            "\n",
            "\n",
            "\n",
            "color image processing\n",
            "\n",
            "} Two major categories of full-color image processing methods\n",
            "\n",
            "} Processing each component image individually and forming a\n",
            "\n",
            "composite processed color image from the individually processed components\n",
            "\n",
            "} Working with color pixels directly\n",
            "\n",
            "} Color pixels are vectors\n",
            "\n",
            "#! #\" ##\n",
            "\n",
            "} ! =\n",
            "\n",
            "#!(', )) #\"(', )) ##(', )) } In order for per-color-component and vector-based processing\n",
            "\n",
            "$(', )) %(', )) &(', ))\n",
            "\n",
            ", ! ', ) =\n",
            "\n",
            "$ % &\n",
            "\n",
            "=\n",
            "\n",
            "=\n",
            "\n",
            "to be equivalent, two conditions have to be satisfied } The process has to be applicable to both vectors and scalars } The operation on each component of a vector must be independent\n",
            "\n",
            "of the other components\n",
            "\n",
            "33\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Spring 2023\n",
            "\n",
            "6.4 Basics of full\n",
            "\n",
            "\n",
            "\n",
            "color image processing\n",
            "\n",
            "} The process of neighborhood averaging\n",
            "\n",
            "1.\n",
            "\n",
            "2.\n",
            "\n",
            "averaging would be accomplished by summing the gray levels of all the pixels in the 2D neighborhood and dividing by the total number of pixels in the neighborhood averaging would be done by summing all the vectors in the 3D neighborhood and dividing each component by the total number of vectors in the neighborhood\n",
            "\n",
            "Figure 6.27 Spatial neighborhoods for grayscale and RGB color images. Observe in (b) that a single pair of spatial coordinates, (3, 5), addresses the same spatial location in all three images.\n",
            "\n",
            "34\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Spring 2023\n",
            "\n",
            "6.5 Color transformations\n",
            "\n",
            "} Processing the components of a color image within the context of a\n",
            "\n",
            "single color model (as opposed to the conversion of those components between models)\n",
            "\n",
            "} Formulation\n",
            "\n",
            "} The pixel values here are triplets or quartets from the color space chosen to represent the images\n",
            "\n",
            "} HSI color space\n",
            "\n",
            "C6 = D6 86 , E = 1, … , G, H !, ( = D I !, ( } RGB: 7\" = 89\", : = 1,2,3\n",
            "\n",
            "Figure 6.28 A full-color image and its various color- space components. (Original image courtesy of MedData Interactive.)\n",
            "\n",
            "35\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Spring 2023\n",
            "\n",
            "6.5 Color transformations\n",
            "\n",
            "} Formulation\n",
            "\n",
            "} CMY/CMYK color space\n",
            "\n",
            "Figure 6.29 Adjusting the intensity of an image using color transformations. (a) Original image. (b) Result of decreasing its intensity by 30% (i.e., letting 7 = 0.7). (c) The required RGB mapping function. (d)–(e) The required CMYK mapping functions. (f) The required CMY mapping function. (g)–(h) The required HSI mapping functions. (Original image courtesy of MedData Interactive.)\n",
            "\n",
            "36\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Spring 2023\n",
            "\n",
            "6.5 Color transformations\n",
            "\n",
            "} Color complements\n",
            "\n",
            "} Complements: the hues directly opposite one another on the\n",
            "\n",
            "color circle\n",
            "\n",
            "} Analogous to the gray-scale negatives\n",
            "\n",
            "Figure 6.30 Color complements on the color circle.\n",
            "\n",
            "37\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Spring 2023\n",
            "\n",
            "6.5 Color transformations\n",
            "\n",
            "} Color complements\n",
            "\n",
            "} Ex. 6.7 (computing color image complements)\n",
            "\n",
            "Figure 6.31 Color complement transformations. (a) Original image. (b) Complement transformation functions. (c) Complement of (a) based on the RGB mapping functions. (d) An approximation of the RGB complement using HSI transformations.\n",
            "\n",
            "38\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Spring 2023\n",
            "\n",
            "6.5 Color transformations } Color slicing\n",
            "\n",
            "} Highlighting a specific range of colors in an image is useful for\n",
            "\n",
            "separating objects from their surroundings } To display the colors of interest so that they stand out from the\n",
            "\n",
            "background or\n",
            "\n",
            "} To use the region defined by the colors as a mask for further\n",
            "\n",
            "processing\n",
            "\n",
            "} One simple way to slice a color image\n",
            "\n",
            "} The transformations highlight the colors around the prototype\n",
            "\n",
            "by forcing all other colors to the midpoint of the reference color space\n",
            "\n",
            "¨ 7\" = =\n",
            "\n",
            "0.5,\n",
            "\n",
            "9\",\n",
            "\n",
            "if\n",
            "\n",
            "9# − @# >\n",
            "\n",
            "$ % &'( )\n",
            "\n",
            "\n",
            "\n",
            "#\n",
            "\n",
            "\n",
            "\n",
            "' otherwise\n",
            "\n",
            ",\n",
            "\n",
            ": = 1, … , C\n",
            "\n",
            "¨ 7\" = =\n",
            "\n",
            "0.5, 9\",\n",
            "\n",
            "' if ∑#+)\n",
            "\n",
            "9# − @#\n",
            "\n",
            "%\n",
            "\n",
            "% , >\n",
            "\n",
            "\n",
            "\n",
            ", otherwise\n",
            "\n",
            ": = 1, … , C\n",
            "\n",
            "39\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Spring 2023\n",
            "\n",
            "6.5 Color transformations\n",
            "\n",
            "} Color slicing\n",
            "\n",
            "} Ex. 6.8 (color slicing)\n",
            "\n",
            "Figure 6.32 Color-slicing transformations that detect (a) reds within an RGB cube of width < = 0.2549 centered at (0.6863, 0.1608, 0.1922), and (b) reds within an RGB sphere of radius 0.1765 centered at the same point. Pixels outside the cube and sphere were replaced by color (0.5, 0.5, 0.5).\n",
            "\n",
            "40\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Spring 2023\n",
            "\n",
            "6.5 Color transformations\n",
            "\n",
            "} Tone and color corrections\n",
            "\n",
            "} The principal benefit of calibrated imaging systems\n",
            "\n",
            "} They allow tonal and color imbalances to be corrected interactively\n",
            "\n",
            "} The tonal range of an image refers to its general distribution of\n",
            "\n",
            "color intensities } High-key images: concentrated at high intensities } Low-key images: located predominantly at low intensities } Middle-key images: lie in between\n",
            "\n",
            "41\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Spring 2023\n",
            "\n",
            "6.5 Color transformations\n",
            "\n",
            "} Tone and color corrections } Ex. 6.9 (tonal transformation) } Tonal corrections for flat, light,\n",
            "\n",
            "and dark color images\n",
            "\n",
            "Figure 6.33 Tonal corrections for flat, light (high key), and dark (low key) color images. Adjusting the red, green, and blue components equally does not always alter the image hues significantly.\n",
            "\n",
            "42\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Spring 2023\n",
            "\n",
            "6.5 Color transformations\n",
            "\n",
            "} Tone and color corrections } Ex. 6.10 (color balancing)\n",
            "\n",
            "} Color balancing corrections\n",
            "\n",
            "for CMYK color images\n",
            "\n",
            "Figure 6.34 Color balancing a C M Y K image.\n",
            "\n",
            "43\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Spring 2023\n",
            "\n",
            "6.5 Color transformations\n",
            "\n",
            "} Histogram processing of color images\n",
            "\n",
            "} It is generally unwise to histogram equalize the components of a color image independently (resulting in erroneous color)\n",
            "\n",
            "} A more logical approach is to spread the color intensities uniformly,\n",
            "\n",
            "leaving the colors themselves (e.g., hues) unchanged\n",
            "\n",
            "} Ex. 6.11 (HE in the HSI color\n",
            "\n",
            "space)\n",
            "\n",
            "Figure 6.35 Histogram equalization (followed by saturation adjustment) in the HSI color space.\n",
            "\n",
            "44\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Spring 2023\n",
            "\n",
            "6.6 Color image smoothing and sharpening\n",
            "\n",
            "} The basics of neighborhood processing are illustrated\n",
            "\n",
            "within the context of color image smoothing and sharpening\n",
            "\n",
            "} Color image smoothing\n",
            "\n",
            "} Smoothing by neighborhood averaging can be carried out on a\n",
            "\n",
            "per\n",
            "\n",
            "\n",
            "\n",
            "color\n",
            "\n",
            "\n",
            "\n",
            "plane basis\n",
            "\n",
            "̅M %, & =\n",
            "\n",
            ")\n",
            "\n",
            "\n",
            "\n",
            "∑ .,0 ∈2)\n",
            "\n",
            "\n",
            "\n",
            "M(7, O) =\n",
            "\n",
            ")\n",
            "\n",
            ")\n",
            "\n",
            ")\n",
            "\n",
            "\n",
            "\n",
            "∑ .,0 ∈2)\n",
            "\n",
            "\n",
            "\n",
            "∑ .,0 ∈2)\n",
            "\n",
            "\n",
            "\n",
            "∑ .,0 ∈2)\n",
            "\n",
            "\n",
            "\n",
            "(7, O)\n",
            "\n",
            "+(7, O)\n",
            "\n",
            ",(7, O)\n",
            "\n",
            "45\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Spring 2023\n",
            "\n",
            "6.6 Color image smoothing and sharpening\n",
            "\n",
            "} Color image smoothing\n",
            "\n",
            "} Ex. 6.12 (color image smoothing by neighborhood averaging)\n",
            "\n",
            "Figure 6.36 (a) RGB image. (b) Red component image. (c)Green component. (d) Blue component.\n",
            "\n",
            "46\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Spring 2023\n",
            "\n",
            "6.6 Color image smoothing and sharpening\n",
            "\n",
            "} Color image smoothing\n",
            "\n",
            "} Ex. 6.12 (color image smoothing by neighborhood averaging)\n",
            "\n",
            "Figure 6.37 HSI components of the RGB color image in Fig. 6.36(a). (a) Hue. (b) Saturation. (c) Intensity.\n",
            "\n",
            "Figure 6.38 Image smoothing with a 5×5 averaging kernel. (a) Result of processing each RGB component image. (b) Result of processing the intensity component of the HSI image and converting to RGB. (c) Difference between the two results.\n",
            "\n",
            "47\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Spring 2023\n",
            "\n",
            "6.6 Color image smoothing and sharpening\n",
            "\n",
            "} Color image sharpening\n",
            "\n",
            "} Ex. 6.13 (image sharpening using the Laplacian)\n",
            "\n",
            "Q% M %, & =\n",
            "\n",
            "Q%*(%, &) Q%+(%, &) Q%,(%, &)\n",
            "\n",
            "Figure 6.39 Image sharpening using the Laplacian. (a) Result of processing each RGB channel. (b) Result of processing the HSI intensity component and converting to RGB. (c) Difference between the two results.\n",
            "\n",
            "48\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Spring 2023\n",
            "\n",
            "6.7 Using color in image segmentation\n",
            "\n",
            "} Segmentation is a process that partitions an image into regions\n",
            "\n",
            "} Segmentation in HSI color space\n",
            "\n",
            "} If we want to carry out the segmentation process on individual planes, it is natural to think first of the HSI space because color is conveniently represented in the hue image\n",
            "\n",
            "} Saturation is used as a masking image in order to isolate\n",
            "\n",
            "further regions of interest in the hue image\n",
            "\n",
            "} The intensity image is used less frequently for segmentation of\n",
            "\n",
            "color images because it carries no color information\n",
            "\n",
            "49\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Spring 2023\n",
            "\n",
            "6.7 Using color in image segmentation\n",
            "\n",
            "} Segmentation in HSI color space\n",
            "\n",
            "} Ex. 6.14 (segmenting a color image in HSI color space)\n",
            "\n",
            "Figure 6.40 Image segmentation in HSI space. (a) Original. (b) Hue. (c) Saturation. (d) Intensity. (e) Binary saturation mask (black = 0). (f) Product of (b) and (e). (g) Histogram of (f). (h) Segmentation of red components from (a).\n",
            "\n",
            "50\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Spring 2023\n",
            "\n",
            "6.7 Using color in image segmentation\n",
            "\n",
            "} Segmentation in RGB space\n",
            "\n",
            "} The objective is to segment objects of a specified color range\n",
            "\n",
            "in an RGB image\n",
            "\n",
            "} Let denote the average color that we wish to segment\n",
            "\n",
            "by a vector 7 } The objective of segmentation is to classify each RGB pixel in a given\n",
            "\n",
            "image as having a color in the specified range or not\n",
            "\n",
            "} It is necessary to have a measure of similarity } Let @ denote an arbitrary point in RGB space R S, T = S − T = S − T 3(S − T)\n",
            "\n",
            "= !4 − @4\n",
            "\n",
            "% + !5 − @5\n",
            "\n",
            "% + !6 − @6\n",
            "\n",
            "%\n",
            "\n",
            "R S, T = S − T = S − T 3V7)(S − T)\n",
            "\n",
            "51\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Spring 2023\n",
            "\n",
            "6.7 Using color in image segmentation\n",
            "\n",
            "} Segmentation in RGB vector space\n",
            "\n",
            "} Three approaches\n",
            "\n",
            "Figure 6.41 Three approaches for enclosing data regions for RGB vector segmentation.\n",
            "\n",
            "} Ex. 6.15 (color segmentation\n",
            "\n",
            "in RGB color space)\n",
            "\n",
            "Figure 6.42 Segmentation in RGB space. (a) Original image with colors of interest shown enclosed by a rectangle. (b) Result of segmentation in RGB vector space. Compare with Fig. 6.40(h).\n",
            "\n",
            "52\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Spring 2023\n",
            "\n",
            "6.7 Using color in image segmentation\n",
            "\n",
            "} Color edge detection\n",
            "\n",
            "} The details of edge-based segmentation are given in sec10.2 } Processing the three individual planes to form a composite\n",
            "\n",
            "gradient image can yield erroneous results\n",
            "\n",
            "} If accuracy is an issue, we need a new definition of the gradient\n",
            "\n",
            "applicable to vector quantities\n",
            "\n",
            "Figure 6.43 (a)–(c) R, G, and B component images, and (d) resulting RGB color image. (e)–(g) R, G, and B component images, and (h) resulting RGB color image.\n",
            "\n",
            "53\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Spring 2023\n",
            "\n",
            "6.7 Using color in image segmentation\n",
            "\n",
            "} Color edge detection\n",
            "\n",
            "} The direction of maximum rate of change of 8(:, <) is given by\n",
            "\n",
            "the angle and the value of the rate of change at :, <\n",
            "\n",
            "W =\n",
            "\n",
            "84 89\n",
            "\n",
            "X +\n",
            "\n",
            "85 89\n",
            "\n",
            "Y +\n",
            "\n",
            "86 89\n",
            "\n",
            "Z, [ =\n",
            "\n",
            "84 8(\n",
            "\n",
            "X +\n",
            "\n",
            "85 8(\n",
            "\n",
            "Y +\n",
            "\n",
            "86 8(\n",
            "\n",
            "Z\n",
            "\n",
            "\\99 = W ] W = W3W =\n",
            "\n",
            "%\n",
            "\n",
            "+\n",
            "\n",
            "84 89\n",
            "\n",
            "85 89\n",
            "\n",
            "%\n",
            "\n",
            "+\n",
            "\n",
            "\\(( = [ ] [ = [3[ =\n",
            "\n",
            "\\9( = W ] [ = W3[ =\n",
            "\n",
            "→ _ %, & =\n",
            "\n",
            "tan7)\n",
            "\n",
            ") %\n",
            "\n",
            "%\n",
            "\n",
            "84\n",
            "\n",
            "+\n",
            "\n",
            "8( 84 89\n",
            "\n",
            "+\n",
            "\n",
            "84 8( %:)\n",
            "\n",
            "\n",
            "\n",
            ":))7:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "%\n",
            "\n",
            "85\n",
            "\n",
            "8( 85 89\n",
            "\n",
            "85 8(\n",
            "\n",
            "+\n",
            "\n",
            "+\n",
            "\n",
            ",\n",
            "\n",
            "%\n",
            "\n",
            "86 89\n",
            "\n",
            "%\n",
            "\n",
            "86\n",
            "\n",
            "8( 86 89\n",
            "\n",
            "86 8(\n",
            "\n",
            "JB !, ( =\n",
            "\n",
            "1 2\n",
            "\n",
            "HCC + HDD + HCC − HDD cos 2N(!, () + 2HCD sin 2N(!, ()\n",
            "\n",
            "54\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Spring 2023\n",
            "\n",
            "6.7 Using color in image segmentation\n",
            "\n",
            "} Color edge detection\n",
            "\n",
            "} Ex. 6.16 (edge detection in RGB color space)\n",
            "\n",
            "Figure 6.44 (a) RGB image. (b) Gradient computed in RGB color vector space. (c) Gradient image formed by the elementwise sum of three individual gradient images, each computed using the Sobel operators. (d) Difference between (b) and (c).\n",
            "\n",
            "55\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Spring 2023\n",
            "\n",
            "6.7 Using color in image segmentation\n",
            "\n",
            "} Color edge detection\n",
            "\n",
            "} Ex. 6.16 (edge detection in RGB color space)\n",
            "\n",
            "Figure 6.45 Component gradient images of the color image in Fig. 6.44. (a) Red component, (b) green component, and (c) blue component. These three images were added and scaled to produce the image in Fig. 6.44(c).\n",
            "\n",
            "56\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Spring 2023\n",
            "\n",
            "6.8 Noise in color images\n",
            "\n",
            "} The noise content of a color image has the same\n",
            "\n",
            "characteristics in each color channel, but it is possible for color channels to be affected differently by noise } Ex. 6.17 (illustration of\n",
            "\n",
            "the effects when converting noisy RGB images to HSI)\n",
            "\n",
            "Figure 6.46 (a)–(c) Red, green, and blue 8- bit component images corrupted by additive Gaussian noise of mean 0 and standard deviation of 28 intensity levels. (d) Resulting RGB image. [Compare (d) with Fig. 6.44(a).]\n",
            "\n",
            "57\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Spring 2023\n",
            "\n",
            "6.8 Noise in color images\n",
            "\n",
            "} Ex. 6.17\n",
            "\n",
            "Figure 6.47 HSI components of the noisy color image in Fig. 6.46(d). (a) Hue. (b) Saturation. (c) Intensity.\n",
            "\n",
            "Figure 6.48 (a) RGB image with green plane corrupted by salt- and-pepper noise. (b) Hue component of HSI image. (c) Saturation component. (d) Intensity component.\n",
            "\n",
            "58\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Spring 2023\n",
            "\n",
            "Summary\n",
            "\n",
            "} Color fundamentals } Color models } Pseudocolor image processing } Basics of full-color image processing } Color transformations } Color image smoothing and sharpening } Using color in image segmentation } Noise in color images } Color image compression\n",
            "\n",
            "59\n",
            "\n",
            "6. Color Image Processing\n",
            "\n",
            "Spring 2023\n"
          ]
        }
      ],
      "source": [
        "print(docs[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dnsc0SxHtXPn"
      },
      "source": [
        "Looks good, we need to also consider the length of each page with respect to the number of tokens that will reasonably fit within the window of a ChatGPT model. We will use `gpt-3.5-turbo` as the assumed model.\n",
        "\n",
        "### Chunking the Text\n",
        "\n",
        "At the time of writing, `gpt-3.5-turbo` supports a context window of 4096 tokens — that means that input tokens + generated ( / completion) output tokens, cannot total more than 4096 without hitting an error.\n",
        "\n",
        "So we 100% need to keep below this. If we assume a very safe margin of ~2000 tokens for the input prompt into `gpt-3.5-turbo`, leaving ~2000 tokens for conversation history and completion.\n",
        "\n",
        "With this ~2000 token limit we may want to include *five* snippets of relevant information, meaning each snippet can be no more than **400** token long.\n",
        "\n",
        "To create these snippets we use the `RecursiveCharacterTextSplitter` from LangChain. To measure the length of snippets we also need a *length function*. This is a function that consumes text, counts the number of tokens within the text (after tokenization using the `gpt-3.5-turbo` tokenizer), and returns that number. We define it like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "u1-4qAiUtXPn"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "\n",
        "tokenizer = tiktoken.get_encoding('cl100k_base')\n",
        "\n",
        "# create the length function\n",
        "def tiktoken_len(text):\n",
        "    tokens = tokenizer.encode(\n",
        "        text,\n",
        "        disallowed_special=()\n",
        "    )\n",
        "    return len(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig7Py5NVtXPn"
      },
      "source": [
        "Note that for the tokenizer we defined the encoder as `\"cl100k_base\"`. This is a specific tiktoken encoder which is used by `gpt-3.5-turbo`. Other encoders exist and at the time of writing are summarized as:\n",
        "\n",
        "| Encoder | Models |\n",
        "| --- | --- |\n",
        "| `cl100k_base` | `gpt-4`, `gpt-3.5-turbo`, `text-embedding-ada-002` |\n",
        "| `p50k_base` | `text-davinci-003`, `code-davinci-002`, `code-cushman-002` |\n",
        "| `r50k_base` | `text-davinci-001`, `davinci`, `text-similarity-davinci-001` |\n",
        "| `gpt2` | `gpt2` |\n",
        "\n",
        "You can find these details in the [Tiktoken `model.py` script](https://github.com/openai/tiktoken/blob/main/tiktoken/model.py), or using `tiktoken.encoding_for_model`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "uqtyb_nRtXPn",
        "outputId": "1799f08d-eb88-415a-8eb4-64b8de919649"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Encoding 'cl100k_base'>"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tiktoken.encoding_for_model('gpt-4') # gpt4로 바꾸기 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLFRqT4rtXPo"
      },
      "source": [
        "With the length function defined we can initialize our `RecursiveCharacterTextSplitter` object like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "S8JX-JF7tXPo"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=400,\n",
        "    chunk_overlap=20,  # number of tokens overlap between chunks\n",
        "    length_function=tiktoken_len,\n",
        "    separators=['\\n\\n', '\\n', ' ', '']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41PQTB4rtXPo"
      },
      "source": [
        "Then we split the text for a document like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "GClNt4lDtXPo",
        "outputId": "0a8b1519-5e5a-487f-9285-a5f567e7b633"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "21"
            ]
          },
          "execution_count": 124,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chunks = text_splitter.split_text(docs[0].page_content # 큰 pdf 를 분리하는 것이고 이는 토큰에 맞게 질문하기 위해서이다.\n",
        "len(chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "rlL_61N0tXPo",
        "outputId": "930afb11-a10c-4d25-f4a7-2e2f112e5c0c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(384, 373)"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tiktoken_len(chunks[0]), tiktoken_len(chunks[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVEhDDyatXPo"
      },
      "source": [
        "For `docs[5]` we created `2` chunks of token length `346` and `247`.\n",
        "\n",
        "This is for a single document, we need to do this over all of our documents. While we iterate through the docs to create these chunks we will reformat them into the format required by our API app. This format needs to align to the `/upsert` endpoints required document format, which looks like this:\n",
        "\n",
        "```json\n",
        "[\n",
        "    {\n",
        "        \"id\": \"abc\",\n",
        "        \"text\": \"some important document text\",\n",
        "        \"metadata\": {\n",
        "            \"field1\": \"optional metadata goes here\",\n",
        "            \"field2\": 54\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"123\",\n",
        "        \"text\": \"some other important text\",\n",
        "        \"metadata\": {\n",
        "            \"field1\": \"another metadata\",\n",
        "            \"field2\": 71,\n",
        "            \"field3\": \"not all metadatas need the same structure\"\n",
        "        }\n",
        "    }\n",
        "    ...\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9iinqjrtXPo"
      },
      "source": [
        "Every document *must* have a `\"text\"` field. The `\"id\"` and `\"metadata\"` fields are optional, however, we will include both.\n",
        "\n",
        "The `\"id\"` will be created based on the URL of the text + it's chunk number."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "uaMqBpsStXPo",
        "outputId": "53600d0e-9556-48f0-d0b0-fdc2b7a7fd8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data/dip.pdf\n",
            "36a81d2f5472\n"
          ]
        }
      ],
      "source": [
        "import hashlib\n",
        "m = hashlib.md5()  # this will convert URL into unique ID\n",
        "\n",
        "url = docs[0].metadata['source'].replace('rtdocs/', 'https://')\n",
        "print(url)\n",
        "\n",
        "# convert URL to unique ID\n",
        "m.update(url.encode('utf-8'))\n",
        "uid = m.hexdigest()[:12]\n",
        "print(uid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xi-dECvftXPo"
      },
      "source": [
        "Then use the `uid` alongside chunk number and actual `url` to create the format needed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "o3BZzCwStXPo",
        "outputId": "990301d3-9fde-441b-a0cc-f94c7869864e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'id': '36a81d2f5472-0',\n",
              "  'text': 'It is only after years of preparation that the young artist should touch color– not color used descriptively, that is, but as a means of personal expression.\\n\\nHenri Matisse\\n\\nFor a long time I limited myself to one color – as a form of discipline.\\n\\nPablo Picasso\\n\\nColor Image Processing\\n\\nChapter 6 어\\n\\nPreview } Motivation of using colors in image processing\\n\\n} A powerful descriptor that simplifies object identification } Thousands of color shades and intensities can be discerned by\\n\\nhumans\\n\\n} Two major areas in color image processing\\n\\n} Full\\n\\n\\n\\ncolor processing\\n\\n} images are acquired with a full-color sensor\\n\\n} Pseudo\\n\\n\\n\\ncolor processing\\n\\n} assigning a color to a particular monochrome intensity or\\n\\nrange of intensities\\n\\n} Full-color image processing techniques are used in broad\\n\\nrange of applications, as the hardware cost is getting reasonable\\n\\n2\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\nPreview\\n\\n} This chapter covers } Color fundamentals } Color models } Pseudocolor image processing } Basics of full-color image processing } Color transformations } Color image smoothing and sharpening } Using color in image segmentation } Noise in color images } Color image compression\\n\\n3\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.1 Color fundamentals\\n\\n} The process followed by the human brain in perceiving\\n\\nand interpreting color is a physiopsychological phenomenon that is not yet fully understood\\n\\n} Sir Issac Newton – color spectrum } The colors that humans and some other animals perceive in an object are determined by the nature of the light reflected from the object\\n\\nFigure 6.1 Color spectrum seen by passing white light through a prism. (Courtesy of the General Electric Co., Lighting Division.)\\n\\n4\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.1 Color fundamentals } Characterization of light is central to the science of color',\n",
              "  'metadata': {'url': 'data/dip.pdf'}},\n",
              " {'id': '36a81d2f5472-1',\n",
              "  'text': '6.1 Color fundamentals } Characterization of light is central to the science of color\\n\\n} Achromatic: the only attribute is its intensity\\n\\n} Chromatic light: the electromagnetic spectrum\\n\\nfrom approximately 400 – 700 %&\\n\\nFigure 6.2 Wavelengths comprising the visible range of the electromagnetic spectrum. (Courtesy of the General Electric Co., Lighting Division.)\\n\\n} Three basic quantities used to describe the quality of a chromatic light source: radiance, luminance, and brightness\\n\\n5\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.1 Color fundamentals } Radiance\\n\\n} The total amount of energy that flows from the light source } It is measured in watts (!)\\n\\n} Luminance\\n\\n} A measure of the amount of energy an observer perceives\\n\\nfrom a light source\\n\\n} It is measured in lumens (\"#)\\n\\n} Brightness\\n\\n} A subjective descriptor that is practically impossible to\\n\\nmeasure\\n\\n} It embodies the achromatic notion of intensity and\\n\\nis one of the key factors in describing color sensation\\n\\n6\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.1 Color fundamentals\\n\\n} Due to the absorption characteristics of the human eye, colors are seen as variable combinations of the primary colors } red (700 &#), green (546.1 &#), and blue (435.8 &#)\\n\\nFigure 6.3 Absorption of light by the red, green, and blue cones in the human eye as a function of wavelength.\\n\\n7\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.1 Color fundamentals\\n\\n} The secondary colors of light\\n\\n} magenta (. + 0), cyan (1 + 0), yellow (. + 1) } Differentiating between the primary colors of light and the\\n\\nprimary colors of pigments or colorants is important',\n",
              "  'metadata': {'url': 'data/dip.pdf'}},\n",
              " {'id': '36a81d2f5472-2',\n",
              "  'text': 'primary colors of pigments or colorants is important\\n\\nFigure 6.4 Primary and secondary colors of light and pigments. (Courtesy of the General Electric Co., Lighting Division.)\\n\\n8\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.1 Color fundamentals } Color TV reception is an example of the additive nature\\n\\nof light colors\\n\\n} The characteristics used to distinguish one color from\\n\\nanother are brightness, hue, and saturation } Hue\\n\\n} An attribute associated with the dominant wavelength in a mixture of light waves; it represents dominant color as perceived by an observer\\n\\n} When we call an object red, orange, or yellow, we are specifying its\\n\\nhue } Saturation\\n\\n} The relative purity or the amount of white light mixed with a hue } The pure spectrum colors are fully saturated } The degree of saturation is inversely proportional to the amount of\\n\\nwhite light added\\n\\n} Chromaticity: hue and saturation\\n\\n9\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.1 Color fundamentals\\n\\n} Tristimulus values\\n\\n} The amounts of red (2), green (3), and blue (4) needed to\\n\\nform any particular color\\n\\n} A color is specified by its trichromatic coefficients (F6.5)\\n\\n! =\\n\\n# # + % + &\\n\\n,\\n\\n( =\\n\\n% # + % + &\\n\\n,\\n\\n) =\\n\\n& # + % + &\\n\\n→ ! + ( + ) = 1\\n\\n} Another approach for specifying colors\\n\\n– CIE chromaticity diagram } It shows color composition as a function of ! (red) and \" (green) } The corresponding value of # (blue) is obtained from the equation\\n\\n! = 1 − % − &\\n\\n10\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.1 Color fundamentals\\n\\n} The chromaticity diagram:\\n\\n} useful for color mixing because a straight-line segment joining any two',\n",
              "  'metadata': {'url': 'data/dip.pdf'}},\n",
              " {'id': '36a81d2f5472-3',\n",
              "  'text': '} useful for color mixing because a straight-line segment joining any two\\n\\npoints in the diagram defines all the different color variations that can be obtained by combining these two colors additively\\n\\n} A typical range of colors (the color gamut) by RGB monitors and\\n\\nprinting devices\\n\\nFigure 6.5 The C I E chromaticity diagram. (Courtesy of the General Electric Co., Lighting Division.)\\n\\nFigure 6.6 Illustrative color gamut of color monitors (triangle) and color printing devices (shaded region).\\n\\n11\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.1 Color fundamentals } 3-CCD/CMOS camera\\n\\nhttps://mvpromedia.com/wp\\n\\n\\n\\ncontent/uploads/2020/02/JAI\\n\\n\\n\\nPress\\n\\n\\n\\nPhoto_Fusion\\n\\n\\n\\nSeries_FS\\n\\n\\n\\n3200T\\n\\n\\n\\n10GE\\n\\n\\n\\nPrism\\n\\n\\n\\nIllus tration_2\\n\\n\\n\\nJPEG\\n\\n\\n\\nformat\\n\\n\\n\\nscaled.jpg\\n\\n12\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.1 Color fundamentals } Bayer Pattern\\n\\n} Color interpolation\\n\\n13\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\nhttps://www.1stvision.com/machine\\n\\n\\n\\nvision\\n\\n\\n\\nsolutions/wp\\n\\n\\n\\ncontent/uploads/2018/04/b ayer\\n\\n\\n\\nvs\\n\\n\\n\\n3cmos\\n\\n\\n\\nprism.png\\n\\n6.2 Color models\\n\\n} The purpose of a color model\\n\\n} To facilitate the specification of colors in some standard way } A specification of a coordinate system and a subspace within that system where each color is represented by a single point\\n\\n} RGB: for color monitors and a broad class of color video\\n\\ncameras\\n\\n} CMY (cyan, magenta, yellow) and CMYK (+black) for color\\n\\nprinting\\n\\n} HSI (hue, saturation, intensity)',\n",
              "  'metadata': {'url': 'data/dip.pdf'}},\n",
              " {'id': '36a81d2f5472-4',\n",
              "  'text': \"printing\\n\\n} HSI (hue, saturation, intensity)\\n\\n} corresponds to the way humans describe and interpret color } it decouples the color and gray-scale information in an image, making it suitable for many of the gray-scale techniques developed in this book\\n\\n} Numerous color models\\n\\n14\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.2 Color models\\n\\n} The RGB color model\\n\\n} Based on a Cartesian coordinate system } The different colors are points on or inside the cube, and are\\n\\ndefined by vector extending from the origin\\n\\n} Pixel depth: the number of bits used to represent each pixel\\n\\nin RGB space } A depth of 24 bits: an RGB image in which each of the red, green, and\\n\\nblue images is an 8-bit image (full-color: 16,777,216)\\n\\nFigure 6.7 Schematic of the RGB color cube. Points along the main diagonal have gray values, from black at the origin to white at point (1, 1, 1).\\n\\nFigure 6.8 A 24\\n\\n\\n\\nbit RGB color cube.\\n\\n15\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.2 Color models\\n\\n} The RGB color model\\n\\n} Ex. 6.1 (generating a cross-section of the RGB color cube and\\n\\nits three hidden planes)\\n\\nFigure 6.9 (a) Generating the RGB image of the cross-sectional color plane (127, G, B). (b) The three hidden surface planes in the color cube of Fig. 6.8.\\n\\n16\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.2 Color models\\n\\n} The CMY and CMYK color models\\n\\n} Most devices that deposit colored pigments on paper, such as\\n\\ncolor printers and copiers, require CMY data input or perform an RGB to CMY conversion internally\\n\\n' ( )\\n\\n=\",\n",
              "  'metadata': {'url': 'data/dip.pdf'}},\n",
              " {'id': '36a81d2f5472-5',\n",
              "  'text': '\\' ( )\\n\\n=\\n\\n1 1 1\\n\\n−\\n\\n+ ,\\n\\n} Equal amount of the pigment primaries, 5, 6, and 3 should\\n\\nproduce black } Combining these colors for printing produces a muddy-looking black } In order to produce true black, a fourth color, black, is added, giving\\n\\nrise to the CMYK color model\\n\\n17\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.2 Color models\\n\\n} The CMY and CMYK color models\\n\\n} Pixelwise operation\\n\\n} Conversion from CMY to CMYK\\n\\n} $ = min(*, ,, -) } If $ = 1 pure black , * = , = - = 0 Otherwise 1 − $ * = * − $, * =\\n\\n1 − $ , = , − $, , =\\n\\n!\"# $\"# %\"# $\"#\\n\\n1 − $\\n\\n\\n\\n=\\n\\n\\n\\n− $,\\n\\n=\\n\\n&\"# $\"#\\n\\n¨ In the rage [0, 1]\\n\\n} Conversion from CMYK to CMY\\n\\n= * 1 − $ + $ , = , 1 − $ + $ - = - 1 − $ + $\\n\\n18\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.2 Color models\\n\\n} The HSI color model\\n\\n} The model decouples the intensity component from the color- carrying information (hue and saturation) in a color image } The model is an ideal tool for developing image processing algorithms based on color descriptions that are natural and intuitive to humans\\n\\n} Extracting intensity from an RGB image\\n\\nFigure 6.10 Conceptual relationships between the RGB and HSI color models.\\n\\n19\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.2 Color models\\n\\n} The HSI color model',\n",
              "  'metadata': {'url': 'data/dip.pdf'}},\n",
              " {'id': '36a81d2f5472-6',\n",
              "  'text': 'Spring 2023\\n\\n6.2 Color models\\n\\n} The HSI color model\\n\\nFigure 6.11 Hue and saturation in the HSI color model. The dot is any color point. The angle from the red axis gives the hue. The length of the vector is the saturation. The intensity of all colors in any of these planes is given by the position of the plane on the vertical intensity axis.\\n\\n20\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\nFigure 6.12 The HSI color model based on (a) triangular, and (b) circular color planes. The triangles and circles are perpendicular to the vertical intensity axis.\\n\\n6.2 Color models } The HSI color model\\n\\n} Converting colors from RGB to HSI\\n\\n} ! = #\\n\\n$, 360 − $,\\n\\nif ( ≤\\n\\n\\n\\nif ( >\\n\\n\\n\\n0ℎ232 $ = cos!\"\\n\\n[ $!% &($!()]\\n\\n! \" $!% \"&($!()(%!()\\n\\n} 7 = 1 −\\n\\n+\\n\\n$&%&(\\n\\nmin ;,\\n\\n\\n\\n, (\\n\\n} < =\\n\\n\"\\n\\n+\\n\\n(; +\\n\\n\\n\\n+ ()\\n\\n} Converting colors from HSI to RGB\\n\\n} There are three sectors of interest, corresponding to the 120° intervals in the\\n\\nseparation of primaries\\n\\n} RG sector (0∘ ≤ ! ≤ 120∘)\\n\\n¨ \\' = ) 1 − , , . = ) 1 +\\n\\n! \"#$ % \"#$ &\\'°(%\\n\\n, 0 = 3) − (. + \\')\\n\\n} GB sector (120∘ ≤ ! ≤ 240∘) ! = ! − 120°\\n\\n¨ . = ) 1 − , , 0 = ) 1 +\\n\\n! \"#$ % \"#$ &\\'°(%\\n\\n, \\' = 3) − (. + 0)',\n",
              "  'metadata': {'url': 'data/dip.pdf'}},\n",
              " {'id': '36a81d2f5472-7',\n",
              "  'text': ', \\' = 3) − (. + 0)\\n\\n} BR sector (240∘ ≤ ! ≤ 360∘) ! = ! − 240°\\n\\n¨ 0 = ) 1 − , , \\' = ) 1 +\\n\\n! \"#$ % \"#$ &\\'°(%\\n\\n, . = 3) − (0 + \\')\\n\\n21\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.2 Color models\\n\\n} The HSI color model\\n\\n} Converting colors from HSI to RGB\\n\\n} Ex. 6.2 (the HSI values corresponding to the image of the RGB color\\n\\ncube)\\n\\nFigure 6.13 HSI components of the image in Fig. 6.8: (a) hue, (b) saturation, and (c) intensity images.\\n\\n22\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.2 Color models\\n\\n} Manipulating HSI component images\\n\\nFigure 6.14 (a) RGB image and the components of its corresponding HSI image: (b) hue, (c) saturation, and (d) intensity.\\n\\nFigure 6.15 (a)-(c) Modified HSI component images. (d) Resulting RGB image. (See Fig. 6.14 for the original HSI images.)\\n\\n23\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.2 Color models } A device independent color model\\n\\n} In conjunction with digital cameras, flatbed scanner, and inkjet\\n\\nprinter, a PC can be turned into a digital darkroom\\n\\n} A device independent color model\\n\\n} Relates the color gamuts of the monitors and output devices } CIELAB\\n\\n,∗ = 116 . ℎ\\n\\n1∗ = 500 ℎ\\n\\n4∗ = 200 ℎ\\n\\n% %/ # #/ 0 0#\\n\\n− 16',\n",
              "  'metadata': {'url': 'data/dip.pdf'}},\n",
              " {'id': '36a81d2f5472-8',\n",
              "  'text': '% %/ # #/ 0 0#\\n\\n− 16\\n\\n− ℎ\\n\\n− ℎ\\n\\n% %/ 1 1#\\n\\n,\\n\\n6ℎ787 ℎ(:) = <\\n\\n$ :,\\n\\n: > 0.008856 7.787: + 16/116, : ≤ 0.008856\\n\\n} While not a directly displayable format, its gamut encompasses the\\n\\nentire visible spectrum and can represent accurately the colors of any device\\n\\n24\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.3 Pseudocolor image processing\\n\\n} Pseudocolor (false color)\\n\\nimage processing consists of assigning colors to gray values based on a specified criterion\\n\\n} For human visualization and interpretation of gray-scale events in an\\n\\nimage or sequence of image\\n\\n} Intensity slicing and color coding\\n\\n} >(!, \") = ?4 to slice the image function into two levels (Fig. 6.16) if 3 %, & ∈ 5! 3 %, & = 6!\\n\\n} Alternate representation (Fig. 6.17)\\n\\nFigure 6.16 Graphical interpretation of the intensity slicing technique.\\n\\nFigure 6.17 An alternative representation of the intensity slicing technique.\\n\\n25\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.3 Pseudocolor image processing\\n\\n} Intensity slicing and color coding\\n\\n} Ex. 6.3 (intensity slicing and color coding)\\n\\n} Regions that appear of constant intensity in the monochrome image are quite variable. The color image shows eight different regions of constant intensity\\n\\nFigure 6.18 (a) Grayscale image of the Picker Thyroid Phantom. (b) Result of intensity slicing using eight colors. (Courtesy of Dr. J. L. Blankenship, Oak Ridge National Laboratory.)\\n\\n26\\n\\n6. Color Image Processing\\n\\nSpring 2023',\n",
              "  'metadata': {'url': 'data/dip.pdf'}},\n",
              " {'id': '36a81d2f5472-9',\n",
              "  'text': '26\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.3 Pseudocolor image processing\\n\\n} Intensity slicing and color coding\\n\\n} Subdivision of gray scale based on physical characteristics\\n\\nof the image\\n\\nFigure 6.19 (a) X-ray image of a weld. (b) Result of color coding. (Original image courtesy of X-T E K Systems, Ltd.)\\n\\n27\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.3 Pseudocolor image processing\\n\\n} Intensity slicing and color coding\\n\\n} Ex. 6.4 (use of color to highlight rainfall levels)\\n\\nFigure 6.20 (a) Grayscale image in which intensity (in the horizontal band shown) corresponds to average monthly rainfall. (b) Colors assigned to intensity values. (c) Color-coded image. (d) Zoom of the South American region. (Courtesy of NASA.)\\n\\n28\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.3 Pseudocolor image processing\\n\\n} Intensity to color transformations\\n\\n} Three independent transformations on the gray level\\n\\nof any input pixel } The method discussed in the previous section is\\n\\na special case of the technique\\n\\nFigure 6.21 Functional block diagram for pseudocolor image processing. Images 2\", 2#, and 2$ are fed into the corresponding red, green, and blue inputs of an RGB color monitor.\\n\\n29\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.3 Pseudocolor image processing\\n\\n} Intensity to color transformations\\n\\n} Ex. 6.5 (using pseudocolor to highlight explosives in X-ray\\n\\nimages) : an airport X-ray scanning system\\n\\nFigure 6.22 Pseudocolor enhancement by using the gray level to color transformations in Fig. 7.23. (Original image courtesy of Dr. Mike Hurwitz, Westinghouse.)',\n",
              "  'metadata': {'url': 'data/dip.pdf'}},\n",
              " {'id': '36a81d2f5472-10',\n",
              "  'text': 'Figure 6.23 Transformation functions used to obtain the pseudocolor images in Fig. 6.22.\\n\\n30\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.3 Pseudocolor image processing\\n\\n} Intensity to color transformations\\n\\n} Multispectral image processing\\n\\n} Different sensors produce individual monochrome images,\\n\\neach in a different spectral band\\n\\n} Additional processing\\n\\n} color balancing, combining images, and selecting the three images for\\n\\ndisplay\\n\\nFigure 6.24 A pseudocolor coding approach using multiple grayscale images. The inputs are grayscale images. The outputs are the three components of an RGB composite image.\\n\\n31\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.3 Pseudocolor image processing\\n\\n} Intensity to color transformations\\n\\n} Ex. 6.6 (color coding of multispectral images)\\n\\nFigure 6.25 (a)–(d) Red (R), green (G), blue (B), and near-infrared (IR) components of a LANDSAT multispectral image of the Washington, D.C. area. (e) RGB color composite image obtained using the IR, G, and B component images. (f) RGB color composite image obtained using the R, IR, and B component images. (Original multispectral images courtesy of NASA.)\\n\\n32\\n\\n6. Color Image Processing\\n\\nFigure 6.26 (a) Pseudocolor rendition of Jupiter Moon Io. (b) A close-up. (Courtesy of NASA.)\\n\\nSpring 2023\\n\\n6.4 Basics of full\\n\\n\\n\\ncolor image processing\\n\\n} Two major categories of full-color image processing methods\\n\\n} Processing each component image individually and forming a\\n\\ncomposite processed color image from the individually processed components\\n\\n} Working with color pixels directly\\n\\n} Color pixels are vectors\\n\\n#! #\" ##\\n\\n} ! =',\n",
              "  'metadata': {'url': 'data/dip.pdf'}},\n",
              " {'id': '36a81d2f5472-11',\n",
              "  'text': '} Working with color pixels directly\\n\\n} Color pixels are vectors\\n\\n#! #\" ##\\n\\n} ! =\\n\\n#!(\\', )) #\"(\\', )) ##(\\', )) } In order for per-color-component and vector-based processing\\n\\n$(\\', )) %(\\', )) &(\\', ))\\n\\n, ! \\', ) =\\n\\n$ % &\\n\\n=\\n\\n=\\n\\nto be equivalent, two conditions have to be satisfied } The process has to be applicable to both vectors and scalars } The operation on each component of a vector must be independent\\n\\nof the other components\\n\\n33\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.4 Basics of full\\n\\n\\n\\ncolor image processing\\n\\n} The process of neighborhood averaging\\n\\n1.\\n\\n2.\\n\\naveraging would be accomplished by summing the gray levels of all the pixels in the 2D neighborhood and dividing by the total number of pixels in the neighborhood averaging would be done by summing all the vectors in the 3D neighborhood and dividing each component by the total number of vectors in the neighborhood\\n\\nFigure 6.27 Spatial neighborhoods for grayscale and RGB color images. Observe in (b) that a single pair of spatial coordinates, (3, 5), addresses the same spatial location in all three images.\\n\\n34\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.5 Color transformations\\n\\n} Processing the components of a color image within the context of a\\n\\nsingle color model (as opposed to the conversion of those components between models)\\n\\n} Formulation\\n\\n} The pixel values here are triplets or quartets from the color space chosen to represent the images\\n\\n} HSI color space\\n\\nC6 = D6 86 , E = 1, … , G, H !, ( = D I !, ( } RGB: 7\" = 89\", : = 1,2,3',\n",
              "  'metadata': {'url': 'data/dip.pdf'}},\n",
              " {'id': '36a81d2f5472-12',\n",
              "  'text': 'Figure 6.28 A full-color image and its various color- space components. (Original image courtesy of MedData Interactive.)\\n\\n35\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.5 Color transformations\\n\\n} Formulation\\n\\n} CMY/CMYK color space\\n\\nFigure 6.29 Adjusting the intensity of an image using color transformations. (a) Original image. (b) Result of decreasing its intensity by 30% (i.e., letting 7 = 0.7). (c) The required RGB mapping function. (d)–(e) The required CMYK mapping functions. (f) The required CMY mapping function. (g)–(h) The required HSI mapping functions. (Original image courtesy of MedData Interactive.)\\n\\n36\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.5 Color transformations\\n\\n} Color complements\\n\\n} Complements: the hues directly opposite one another on the\\n\\ncolor circle\\n\\n} Analogous to the gray-scale negatives\\n\\nFigure 6.30 Color complements on the color circle.\\n\\n37\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.5 Color transformations\\n\\n} Color complements\\n\\n} Ex. 6.7 (computing color image complements)\\n\\nFigure 6.31 Color complement transformations. (a) Original image. (b) Complement transformation functions. (c) Complement of (a) based on the RGB mapping functions. (d) An approximation of the RGB complement using HSI transformations.\\n\\n38\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.5 Color transformations } Color slicing\\n\\n} Highlighting a specific range of colors in an image is useful for\\n\\nseparating objects from their surroundings } To display the colors of interest so that they stand out from the\\n\\nbackground or\\n\\n} To use the region defined by the colors as a mask for further\\n\\nprocessing',\n",
              "  'metadata': {'url': 'data/dip.pdf'}},\n",
              " {'id': '36a81d2f5472-13',\n",
              "  'text': 'background or\\n\\n} To use the region defined by the colors as a mask for further\\n\\nprocessing\\n\\n} One simple way to slice a color image\\n\\n} The transformations highlight the colors around the prototype\\n\\nby forcing all other colors to the midpoint of the reference color space\\n\\n¨ 7\" = =\\n\\n0.5,\\n\\n9\",\\n\\nif\\n\\n9# − @# >\\n\\n$ % &\\'( )\\n\\n\\n\\n#\\n\\n\\n\\n\\' otherwise\\n\\n,\\n\\n: = 1, … , C\\n\\n¨ 7\" = =\\n\\n0.5, 9\",\\n\\n\\' if ∑#+)\\n\\n9# − @#\\n\\n%\\n\\n% , >\\n\\n\\n\\n, otherwise\\n\\n: = 1, … , C\\n\\n39\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.5 Color transformations\\n\\n} Color slicing\\n\\n} Ex. 6.8 (color slicing)\\n\\nFigure 6.32 Color-slicing transformations that detect (a) reds within an RGB cube of width < = 0.2549 centered at (0.6863, 0.1608, 0.1922), and (b) reds within an RGB sphere of radius 0.1765 centered at the same point. Pixels outside the cube and sphere were replaced by color (0.5, 0.5, 0.5).\\n\\n40\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.5 Color transformations\\n\\n} Tone and color corrections\\n\\n} The principal benefit of calibrated imaging systems\\n\\n} They allow tonal and color imbalances to be corrected interactively\\n\\n} The tonal range of an image refers to its general distribution of\\n\\ncolor intensities } High-key images: concentrated at high intensities } Low-key images: located predominantly at low intensities } Middle-key images: lie in between\\n\\n41\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.5 Color transformations',\n",
              "  'metadata': {'url': 'data/dip.pdf'}},\n",
              " {'id': '36a81d2f5472-14',\n",
              "  'text': '41\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.5 Color transformations\\n\\n} Tone and color corrections } Ex. 6.9 (tonal transformation) } Tonal corrections for flat, light,\\n\\nand dark color images\\n\\nFigure 6.33 Tonal corrections for flat, light (high key), and dark (low key) color images. Adjusting the red, green, and blue components equally does not always alter the image hues significantly.\\n\\n42\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.5 Color transformations\\n\\n} Tone and color corrections } Ex. 6.10 (color balancing)\\n\\n} Color balancing corrections\\n\\nfor CMYK color images\\n\\nFigure 6.34 Color balancing a C M Y K image.\\n\\n43\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.5 Color transformations\\n\\n} Histogram processing of color images\\n\\n} It is generally unwise to histogram equalize the components of a color image independently (resulting in erroneous color)\\n\\n} A more logical approach is to spread the color intensities uniformly,\\n\\nleaving the colors themselves (e.g., hues) unchanged\\n\\n} Ex. 6.11 (HE in the HSI color\\n\\nspace)\\n\\nFigure 6.35 Histogram equalization (followed by saturation adjustment) in the HSI color space.\\n\\n44\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.6 Color image smoothing and sharpening\\n\\n} The basics of neighborhood processing are illustrated\\n\\nwithin the context of color image smoothing and sharpening\\n\\n} Color image smoothing\\n\\n} Smoothing by neighborhood averaging can be carried out on a\\n\\nper\\n\\n\\n\\ncolor\\n\\n\\n\\nplane basis\\n\\n̅M %, & =\\n\\n)\\n\\n\\n\\n∑ .,0 ∈2)\\n\\n\\n\\nM(7, O) =\\n\\n)\\n\\n)\\n\\n)\\n\\n\\n\\n∑ .,0 ∈2)',\n",
              "  'metadata': {'url': 'data/dip.pdf'}},\n",
              " {'id': '36a81d2f5472-15',\n",
              "  'text': ')\\n\\n)\\n\\n)\\n\\n\\n\\n∑ .,0 ∈2)\\n\\n\\n\\n∑ .,0 ∈2)\\n\\n\\n\\n∑ .,0 ∈2)\\n\\n\\n\\n(7, O)\\n\\n+(7, O)\\n\\n,(7, O)\\n\\n45\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.6 Color image smoothing and sharpening\\n\\n} Color image smoothing\\n\\n} Ex. 6.12 (color image smoothing by neighborhood averaging)\\n\\nFigure 6.36 (a) RGB image. (b) Red component image. (c)Green component. (d) Blue component.\\n\\n46\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.6 Color image smoothing and sharpening\\n\\n} Color image smoothing\\n\\n} Ex. 6.12 (color image smoothing by neighborhood averaging)\\n\\nFigure 6.37 HSI components of the RGB color image in Fig. 6.36(a). (a) Hue. (b) Saturation. (c) Intensity.\\n\\nFigure 6.38 Image smoothing with a 5×5 averaging kernel. (a) Result of processing each RGB component image. (b) Result of processing the intensity component of the HSI image and converting to RGB. (c) Difference between the two results.\\n\\n47\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.6 Color image smoothing and sharpening\\n\\n} Color image sharpening\\n\\n} Ex. 6.13 (image sharpening using the Laplacian)\\n\\nQ% M %, & =\\n\\nQ%*(%, &) Q%+(%, &) Q%,(%, &)\\n\\nFigure 6.39 Image sharpening using the Laplacian. (a) Result of processing each RGB channel. (b) Result of processing the HSI intensity component and converting to RGB. (c) Difference between the two results.\\n\\n48\\n\\n6. Color Image Processing',\n",
              "  'metadata': {'url': 'data/dip.pdf'}},\n",
              " {'id': '36a81d2f5472-16',\n",
              "  'text': '48\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.7 Using color in image segmentation\\n\\n} Segmentation is a process that partitions an image into regions\\n\\n} Segmentation in HSI color space\\n\\n} If we want to carry out the segmentation process on individual planes, it is natural to think first of the HSI space because color is conveniently represented in the hue image\\n\\n} Saturation is used as a masking image in order to isolate\\n\\nfurther regions of interest in the hue image\\n\\n} The intensity image is used less frequently for segmentation of\\n\\ncolor images because it carries no color information\\n\\n49\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.7 Using color in image segmentation\\n\\n} Segmentation in HSI color space\\n\\n} Ex. 6.14 (segmenting a color image in HSI color space)\\n\\nFigure 6.40 Image segmentation in HSI space. (a) Original. (b) Hue. (c) Saturation. (d) Intensity. (e) Binary saturation mask (black = 0). (f) Product of (b) and (e). (g) Histogram of (f). (h) Segmentation of red components from (a).\\n\\n50\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.7 Using color in image segmentation\\n\\n} Segmentation in RGB space\\n\\n} The objective is to segment objects of a specified color range\\n\\nin an RGB image\\n\\n} Let denote the average color that we wish to segment\\n\\nby a vector 7 } The objective of segmentation is to classify each RGB pixel in a given\\n\\nimage as having a color in the specified range or not\\n\\n} It is necessary to have a measure of similarity } Let @ denote an arbitrary point in RGB space R S, T = S − T = S − T 3(S − T)\\n\\n= !4 − @4\\n\\n% + !5 − @5',\n",
              "  'metadata': {'url': 'data/dip.pdf'}},\n",
              " {'id': '36a81d2f5472-17',\n",
              "  'text': '= !4 − @4\\n\\n% + !5 − @5\\n\\n% + !6 − @6\\n\\n%\\n\\nR S, T = S − T = S − T 3V7)(S − T)\\n\\n51\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.7 Using color in image segmentation\\n\\n} Segmentation in RGB vector space\\n\\n} Three approaches\\n\\nFigure 6.41 Three approaches for enclosing data regions for RGB vector segmentation.\\n\\n} Ex. 6.15 (color segmentation\\n\\nin RGB color space)\\n\\nFigure 6.42 Segmentation in RGB space. (a) Original image with colors of interest shown enclosed by a rectangle. (b) Result of segmentation in RGB vector space. Compare with Fig. 6.40(h).\\n\\n52\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.7 Using color in image segmentation\\n\\n} Color edge detection\\n\\n} The details of edge-based segmentation are given in sec10.2 } Processing the three individual planes to form a composite\\n\\ngradient image can yield erroneous results\\n\\n} If accuracy is an issue, we need a new definition of the gradient\\n\\napplicable to vector quantities\\n\\nFigure 6.43 (a)–(c) R, G, and B component images, and (d) resulting RGB color image. (e)–(g) R, G, and B component images, and (h) resulting RGB color image.\\n\\n53\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.7 Using color in image segmentation\\n\\n} Color edge detection\\n\\n} The direction of maximum rate of change of 8(:, <) is given by\\n\\nthe angle and the value of the rate of change at :, <\\n\\nW =\\n\\n84 89\\n\\nX +\\n\\n85 89\\n\\nY +\\n\\n86 89\\n\\nZ, [ =\\n\\n84 8(\\n\\nX +\\n\\n85 8(\\n\\nY +',\n",
              "  'metadata': {'url': 'data/dip.pdf'}},\n",
              " {'id': '36a81d2f5472-18',\n",
              "  'text': 'Z, [ =\\n\\n84 8(\\n\\nX +\\n\\n85 8(\\n\\nY +\\n\\n86 8(\\n\\nZ\\n\\n\\\\99 = W ] W = W3W =\\n\\n%\\n\\n+\\n\\n84 89\\n\\n85 89\\n\\n%\\n\\n+\\n\\n\\\\(( = [ ] [ = [3[ =\\n\\n\\\\9( = W ] [ = W3[ =\\n\\n→ _ %, & =\\n\\ntan7)\\n\\n) %\\n\\n%\\n\\n84\\n\\n+\\n\\n8( 84 89\\n\\n+\\n\\n84 8( %:)\\n\\n\\n\\n:))7:\\n\\n\\n\\n\\n\\n%\\n\\n85\\n\\n8( 85 89\\n\\n85 8(\\n\\n+\\n\\n+\\n\\n,\\n\\n%\\n\\n86 89\\n\\n%\\n\\n86\\n\\n8( 86 89\\n\\n86 8(\\n\\nJB !, ( =\\n\\n1 2\\n\\nHCC + HDD + HCC − HDD cos 2N(!, () + 2HCD sin 2N(!, ()\\n\\n54\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.7 Using color in image segmentation\\n\\n} Color edge detection\\n\\n} Ex. 6.16 (edge detection in RGB color space)\\n\\nFigure 6.44 (a) RGB image. (b) Gradient computed in RGB color vector space. (c) Gradient image formed by the elementwise sum of three individual gradient images, each computed using the Sobel operators. (d) Difference between (b) and (c).\\n\\n55\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.7 Using color in image segmentation\\n\\n} Color edge detection\\n\\n} Ex. 6.16 (edge detection in RGB color space)',\n",
              "  'metadata': {'url': 'data/dip.pdf'}},\n",
              " {'id': '36a81d2f5472-19',\n",
              "  'text': '} Color edge detection\\n\\n} Ex. 6.16 (edge detection in RGB color space)\\n\\nFigure 6.45 Component gradient images of the color image in Fig. 6.44. (a) Red component, (b) green component, and (c) blue component. These three images were added and scaled to produce the image in Fig. 6.44(c).\\n\\n56\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.8 Noise in color images\\n\\n} The noise content of a color image has the same\\n\\ncharacteristics in each color channel, but it is possible for color channels to be affected differently by noise } Ex. 6.17 (illustration of\\n\\nthe effects when converting noisy RGB images to HSI)\\n\\nFigure 6.46 (a)–(c) Red, green, and blue 8- bit component images corrupted by additive Gaussian noise of mean 0 and standard deviation of 28 intensity levels. (d) Resulting RGB image. [Compare (d) with Fig. 6.44(a).]\\n\\n57\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.8 Noise in color images\\n\\n} Ex. 6.17\\n\\nFigure 6.47 HSI components of the noisy color image in Fig. 6.46(d). (a) Hue. (b) Saturation. (c) Intensity.\\n\\nFigure 6.48 (a) RGB image with green plane corrupted by salt- and-pepper noise. (b) Hue component of HSI image. (c) Saturation component. (d) Intensity component.\\n\\n58\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\nSummary\\n\\n} Color fundamentals } Color models } Pseudocolor image processing } Basics of full-color image processing } Color transformations } Color image smoothing and sharpening } Using color in image segmentation } Noise in color images } Color image compression',\n",
              "  'metadata': {'url': 'data/dip.pdf'}},\n",
              " {'id': '36a81d2f5472-20',\n",
              "  'text': '59\\n\\n6. Color Image Processing\\n\\nSpring 2023',\n",
              "  'metadata': {'url': 'data/dip.pdf'}}]"
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = [\n",
        "    {\n",
        "        'id': f'{uid}-{i}',\n",
        "        'text': chunk,\n",
        "        'metadata': {'url': url}\n",
        "    } for i, chunk in enumerate(chunks)\n",
        "]\n",
        "data\n",
        "# 각 pdf를 split 해서 해당하는 곳으로 저장하낟. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_cID2cktXPp"
      },
      "source": [
        "Now we repeat the same logic across our full dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "afa77c267fb14505851d3ff0f8bb7f1a"
          ]
        },
        "id": "0uSz_lNwtXPp",
        "outputId": "93985b12-e043-4fad-d39f-800d19a20ace"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 38.62it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'id': 'a2002610de6a-0', 'text': 'It is only after years of preparation that the young artist should touch color– not color used descriptively, that is, but as a means of personal expression.\\n\\nHenri Matisse\\n\\nFor a long time I limited myself to one color – as a form of discipline.\\n\\nPablo Picasso\\n\\nColor Image Processing\\n\\nChapter 6 어\\n\\nPreview } Motivation of using colors in image processing\\n\\n} A powerful descriptor that simplifies object identification } Thousands of color shades and intensities can be discerned by\\n\\nhumans\\n\\n} Two major areas in color image processing\\n\\n} Full\\n\\n\\n\\ncolor processing\\n\\n} images are acquired with a full-color sensor\\n\\n} Pseudo\\n\\n\\n\\ncolor processing\\n\\n} assigning a color to a particular monochrome intensity or\\n\\nrange of intensities\\n\\n} Full-color image processing techniques are used in broad\\n\\nrange of applications, as the hardware cost is getting reasonable\\n\\n2\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\nPreview\\n\\n} This chapter covers } Color fundamentals } Color models } Pseudocolor image processing } Basics of full-color image processing } Color transformations } Color image smoothing and sharpening } Using color in image segmentation } Noise in color images } Color image compression\\n\\n3\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.1 Color fundamentals\\n\\n} The process followed by the human brain in perceiving\\n\\nand interpreting color is a physiopsychological phenomenon that is not yet fully understood\\n\\n} Sir Issac Newton – color spectrum } The colors that humans and some other animals perceive in an object are determined by the nature of the light reflected from the object\\n\\nFigure 6.1 Color spectrum seen by passing white light through a prism. (Courtesy of the General Electric Co., Lighting Division.)\\n\\n4\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.1 Color fundamentals } Characterization of light is central to the science of color', 'metadata': {'url': 'data/dip.pdf'}}, {'id': 'a2002610de6a-1', 'text': '6.1 Color fundamentals } Characterization of light is central to the science of color\\n\\n} Achromatic: the only attribute is its intensity\\n\\n} Chromatic light: the electromagnetic spectrum\\n\\nfrom approximately 400 – 700 %&\\n\\nFigure 6.2 Wavelengths comprising the visible range of the electromagnetic spectrum. (Courtesy of the General Electric Co., Lighting Division.)\\n\\n} Three basic quantities used to describe the quality of a chromatic light source: radiance, luminance, and brightness\\n\\n5\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.1 Color fundamentals } Radiance\\n\\n} The total amount of energy that flows from the light source } It is measured in watts (!)\\n\\n} Luminance\\n\\n} A measure of the amount of energy an observer perceives\\n\\nfrom a light source\\n\\n} It is measured in lumens (\"#)\\n\\n} Brightness\\n\\n} A subjective descriptor that is practically impossible to\\n\\nmeasure\\n\\n} It embodies the achromatic notion of intensity and\\n\\nis one of the key factors in describing color sensation\\n\\n6\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.1 Color fundamentals\\n\\n} Due to the absorption characteristics of the human eye, colors are seen as variable combinations of the primary colors } red (700 &#), green (546.1 &#), and blue (435.8 &#)\\n\\nFigure 6.3 Absorption of light by the red, green, and blue cones in the human eye as a function of wavelength.\\n\\n7\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.1 Color fundamentals\\n\\n} The secondary colors of light\\n\\n} magenta (. + 0), cyan (1 + 0), yellow (. + 1) } Differentiating between the primary colors of light and the\\n\\nprimary colors of pigments or colorants is important', 'metadata': {'url': 'data/dip.pdf'}}]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "documents = []\n",
        "\n",
        "for doc in tqdm(docs):\n",
        "    url = doc.metadata['source']\n",
        "    m.update(url.encode('utf-8'))\n",
        "    uid = m.hexdigest()[:12]\n",
        "    \n",
        "    chunks = text_splitter.split_text(doc.page_content)\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        documents.append({\n",
        "            'id': f'{uid}-{i}',\n",
        "            'text': chunk,\n",
        "            'metadata': {'url': url}\n",
        "        })\n",
        "\n",
        "len(documents)\n",
        "print(documents[0:2])\n",
        "#총 21개 보유"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNurZ-CPtXPp"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'id': 'a2002610de6a-0',\n",
              "  'text': 'It is only after years of preparation that the young artist should touch color– not color used descriptively, that is, but as a means of personal expression.\\n\\nHenri Matisse\\n\\nFor a long time I limited myself to one color – as a form of discipline.\\n\\nPablo Picasso\\n\\nColor Image Processing\\n\\nChapter 6 어\\n\\nPreview } Motivation of using colors in image processing\\n\\n} A powerful descriptor that simplifies object identification } Thousands of color shades and intensities can be discerned by\\n\\nhumans\\n\\n} Two major areas in color image processing\\n\\n} Full\\n\\n\\n\\ncolor processing\\n\\n} images are acquired with a full-color sensor\\n\\n} Pseudo\\n\\n\\n\\ncolor processing\\n\\n} assigning a color to a particular monochrome intensity or\\n\\nrange of intensities\\n\\n} Full-color image processing techniques are used in broad\\n\\nrange of applications, as the hardware cost is getting reasonable\\n\\n2\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\nPreview\\n\\n} This chapter covers } Color fundamentals } Color models } Pseudocolor image processing } Basics of full-color image processing } Color transformations } Color image smoothing and sharpening } Using color in image segmentation } Noise in color images } Color image compression\\n\\n3\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.1 Color fundamentals\\n\\n} The process followed by the human brain in perceiving\\n\\nand interpreting color is a physiopsychological phenomenon that is not yet fully understood\\n\\n} Sir Issac Newton – color spectrum } The colors that humans and some other animals perceive in an object are determined by the nature of the light reflected from the object\\n\\nFigure 6.1 Color spectrum seen by passing white light through a prism. (Courtesy of the General Electric Co., Lighting Division.)\\n\\n4\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.1 Color fundamentals } Characterization of light is central to the science of color',\n",
              "  'metadata': {'url': 'data/dip.pdf'}},\n",
              " {'id': 'a2002610de6a-1',\n",
              "  'text': '6.1 Color fundamentals } Characterization of light is central to the science of color\\n\\n} Achromatic: the only attribute is its intensity\\n\\n} Chromatic light: the electromagnetic spectrum\\n\\nfrom approximately 400 – 700 %&\\n\\nFigure 6.2 Wavelengths comprising the visible range of the electromagnetic spectrum. (Courtesy of the General Electric Co., Lighting Division.)\\n\\n} Three basic quantities used to describe the quality of a chromatic light source: radiance, luminance, and brightness\\n\\n5\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.1 Color fundamentals } Radiance\\n\\n} The total amount of energy that flows from the light source } It is measured in watts (!)\\n\\n} Luminance\\n\\n} A measure of the amount of energy an observer perceives\\n\\nfrom a light source\\n\\n} It is measured in lumens (\"#)\\n\\n} Brightness\\n\\n} A subjective descriptor that is practically impossible to\\n\\nmeasure\\n\\n} It embodies the achromatic notion of intensity and\\n\\nis one of the key factors in describing color sensation\\n\\n6\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.1 Color fundamentals\\n\\n} Due to the absorption characteristics of the human eye, colors are seen as variable combinations of the primary colors } red (700 &#), green (546.1 &#), and blue (435.8 &#)\\n\\nFigure 6.3 Absorption of light by the red, green, and blue cones in the human eye as a function of wavelength.\\n\\n7\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.1 Color fundamentals\\n\\n} The secondary colors of light\\n\\n} magenta (. + 0), cyan (1 + 0), yellow (. + 1) } Differentiating between the primary colors of light and the\\n\\nprimary colors of pigments or colorants is important',\n",
              "  'metadata': {'url': 'data/dip.pdf'}},\n",
              " {'id': 'a2002610de6a-2',\n",
              "  'text': 'primary colors of pigments or colorants is important\\n\\nFigure 6.4 Primary and secondary colors of light and pigments. (Courtesy of the General Electric Co., Lighting Division.)\\n\\n8\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.1 Color fundamentals } Color TV reception is an example of the additive nature\\n\\nof light colors\\n\\n} The characteristics used to distinguish one color from\\n\\nanother are brightness, hue, and saturation } Hue\\n\\n} An attribute associated with the dominant wavelength in a mixture of light waves; it represents dominant color as perceived by an observer\\n\\n} When we call an object red, orange, or yellow, we are specifying its\\n\\nhue } Saturation\\n\\n} The relative purity or the amount of white light mixed with a hue } The pure spectrum colors are fully saturated } The degree of saturation is inversely proportional to the amount of\\n\\nwhite light added\\n\\n} Chromaticity: hue and saturation\\n\\n9\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.1 Color fundamentals\\n\\n} Tristimulus values\\n\\n} The amounts of red (2), green (3), and blue (4) needed to\\n\\nform any particular color\\n\\n} A color is specified by its trichromatic coefficients (F6.5)\\n\\n! =\\n\\n# # + % + &\\n\\n,\\n\\n( =\\n\\n% # + % + &\\n\\n,\\n\\n) =\\n\\n& # + % + &\\n\\n→ ! + ( + ) = 1\\n\\n} Another approach for specifying colors\\n\\n– CIE chromaticity diagram } It shows color composition as a function of ! (red) and \" (green) } The corresponding value of # (blue) is obtained from the equation\\n\\n! = 1 − % − &\\n\\n10\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.1 Color fundamentals\\n\\n} The chromaticity diagram:\\n\\n} useful for color mixing because a straight-line segment joining any two',\n",
              "  'metadata': {'url': 'data/dip.pdf'}},\n",
              " {'id': 'a2002610de6a-3',\n",
              "  'text': '} useful for color mixing because a straight-line segment joining any two\\n\\npoints in the diagram defines all the different color variations that can be obtained by combining these two colors additively\\n\\n} A typical range of colors (the color gamut) by RGB monitors and\\n\\nprinting devices\\n\\nFigure 6.5 The C I E chromaticity diagram. (Courtesy of the General Electric Co., Lighting Division.)\\n\\nFigure 6.6 Illustrative color gamut of color monitors (triangle) and color printing devices (shaded region).\\n\\n11\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.1 Color fundamentals } 3-CCD/CMOS camera\\n\\nhttps://mvpromedia.com/wp\\n\\n\\n\\ncontent/uploads/2020/02/JAI\\n\\n\\n\\nPress\\n\\n\\n\\nPhoto_Fusion\\n\\n\\n\\nSeries_FS\\n\\n\\n\\n3200T\\n\\n\\n\\n10GE\\n\\n\\n\\nPrism\\n\\n\\n\\nIllus tration_2\\n\\n\\n\\nJPEG\\n\\n\\n\\nformat\\n\\n\\n\\nscaled.jpg\\n\\n12\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\n6.1 Color fundamentals } Bayer Pattern\\n\\n} Color interpolation\\n\\n13\\n\\n6. Color Image Processing\\n\\nSpring 2023\\n\\nhttps://www.1stvision.com/machine\\n\\n\\n\\nvision\\n\\n\\n\\nsolutions/wp\\n\\n\\n\\ncontent/uploads/2018/04/b ayer\\n\\n\\n\\nvs\\n\\n\\n\\n3cmos\\n\\n\\n\\nprism.png\\n\\n6.2 Color models\\n\\n} The purpose of a color model\\n\\n} To facilitate the specification of colors in some standard way } A specification of a coordinate system and a subspace within that system where each color is represented by a single point\\n\\n} RGB: for color monitors and a broad class of color video\\n\\ncameras\\n\\n} CMY (cyan, magenta, yellow) and CMYK (+black) for color\\n\\nprinting\\n\\n} HSI (hue, saturation, intensity)',\n",
              "  'metadata': {'url': 'data/dip.pdf'}}]"
            ]
          },
          "execution_count": 132,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documents[0:4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwwW8lpCtXPp"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJg8SYeUtXPp"
      },
      "source": [
        "### Indexing the Docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_2pS_qotXPq"
      },
      "source": [
        "We're now ready to begin indexing (or *upserting*) our `documents`. To make these requests to the retrieval app API, we will need to provide authorization in the form of the `BEARER_TOKEN` we set earlier. We do this below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "pizW_qqptXPq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "#이건 사용자마자 jwt 토큰 받는거 이거를 헤더에 넣고 \n",
        "BEARER_TOKEN = os.environ.get(\"BEARER_TOKEN\") or \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQ.a-V-4lL-gcmOwXWPR9JCAVfyTmP5usWCHbDRVBAHA58\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PWlNca8tXPq"
      },
      "source": [
        "Use the `BEARER_TOKEN` to create our authorization `headers`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "DHjvFii7tXPq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQ.a-V-4lL-gcmOwXWPR9JCAVfyTmP5usWCHbDRVBAHA58\n"
          ]
        }
      ],
      "source": [
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {BEARER_TOKEN}\"\n",
        "}\n",
        "print(BEARER_TOKEN)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rn6DbxuVtXPq"
      },
      "source": [
        "We'll perform the upsert in batches of `batch_size`. Make sure that the `endpoint_url` variable is set to the correct location for your running *retrieval-app* API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "6ab224fc238f4de2865fb08566df6fea",
            "59aa9111efc44cad8cc4cdb039d8a183",
            "3f563339024a4287bbbb20f85b313f7c",
            "e83fc8770c3d4a9a9e0c2d1198db66d9",
            "22b630a8792742db859dfcd03026d1b0",
            "df8d4737c49347de98add5c2fc426f35",
            "13f11b0a05d7466d9edd84f06e56b687",
            "090ab955bf40499d834929e9bf8da0bb",
            "159419e6e6024fc5891ef97e43dce6a7",
            "97bea5162b9f4ee082f2dd2cd411ce89",
            "6277c6e76ea34522b4b4231194cb5634"
          ]
        },
        "id": "DzlPVM0vtXPq",
        "outputId": "cfb53eaf-d6f9-4aff-9df2-ac7a29efdc68"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:11<00:00, 11.94s/it]\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from requests.adapters import HTTPAdapter, Retry\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "batch_size = 100\n",
        "endpoint_url = \"http://localhost:8000\"\n",
        "s = requests.Session()\n",
        "\n",
        "# we setup a retry strategy to retry on 5xx errors\n",
        "retries = Retry(\n",
        "    total=5,  # number of retries before raising error\n",
        "    backoff_factor=0.1,\n",
        "    status_forcelist=[500, 502, 503, 504]\n",
        ")\n",
        "s.mount('http://', HTTPAdapter(max_retries=retries))\n",
        "\n",
        "for i in tqdm(range(0, len(documents), batch_size)):\n",
        "    i_end = min(len(documents), i+batch_size)\n",
        "    # make post request that allows up to 5 retries\n",
        "    res = s.post(\n",
        "        f\"{endpoint_url}/upsert\",\n",
        "        headers=headers,\n",
        "        json={\n",
        "            \"documents\": documents[i:i_end]\n",
        "        }\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMZiWe2YtXPq"
      },
      "source": [
        "With that our LangChain doc records have all been indexed and we can move on to querying."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7UXES-itXPq"
      },
      "source": [
        "### Making Queries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFFdoM6itXPq"
      },
      "source": [
        "To query the datastore all we need to do is pass one or more queries to the `/query` endpoint. We can make a few questions related to LangChain and see if we return relevant info:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBVguntVtXPq",
        "outputId": "60cf68c6-6b59-470d-f657-1657b8ddb119"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Response [200]>"
            ]
          },
          "execution_count": 139,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "queries = [\n",
        "    {'query': \"What is the color image?\"},\n",
        "    {'query': \"How do I use zzmap?\"},\n",
        "    {'query': \"What is the difference image processing?\"}\n",
        "]\n",
        "\n",
        "res = requests.post(\n",
        "    f\"{endpoint_url}/query\",\n",
        "    headers=headers,\n",
        "    json={\n",
        "        'queries': queries\n",
        "    }\n",
        ")\n",
        "res\n",
        "\n",
        "# 200 - 성공했다는 뜻 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrqLTYuQtXPq"
      },
      "source": [
        "Now we can loop through the responses and see the results returned for each query:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTFRZ2MbtXPq",
        "outputId": "a46b8f84-e557-4734-afea-80d7560ae655"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------------\n",
            "What is the color image?\n",
            "\n",
            "0.89: Color Image Processing\n",
            "0.85: printing  } HSI (hue, saturation, intensity)  } corresponds to the way humans describe and interpret color } it decouples the color and gray-scale information in an image, making it suitable for many of the gray-scale techniques developed in this book  } Numerous color models  14  6. Color Image Processing  Spring 2023  6.2 Color models  } The RGB color model  } Based on a Cartesian coordinate system } The different colors are points on or inside the cube, and are  defined by vector extending from the origin  } Pixel depth: the number of bits used to represent each pixel  in RGB space } A depth of 24 bits: an RGB image in which each of the red, green, and  blue images is an 8-bit image (full-color: 16,777,216)  Figure 6.7 Schematic of the RGB color cube.\n",
            "0.85: 2549 centered at (0.6863, 0.1608, 0.1922), and (b) reds within an RGB sphere of radius 0.1765 centered at the same point. Pixels outside the cube and sphere were replaced by color (0.5, 0.5, 0.5).  40  6. Color Image Processing  Spring 2023  6.5 Color transformations  } Tone and color corrections  } The principal benefit of calibrated imaging systems  } They allow tonal and color imbalances to be corrected interactively  } The tonal range of an image refers to its general distribution of  color intensities } High-key images: concentrated at high intensities } Low-key images: located predominantly at low intensities } Middle-key images: lie in between  41  6. Color Image Processing  Spring 2023  6.\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "How do I use zzmap?\n",
            "\n",
            "0.74: Z, [ =  84 8(  X +  85 8(  Y +  86 8(  Z  \\99 = W ] W = W3W =  %  +  84 89  85 89  %  +  \\(( = [ ] [ = [3[ =  \\9( = W ] [ = W3[ =  → _ %, & =  tan7)  ) %  %  84  +  8( 84 89  +  84 8( %:)    :))7:      %  85  8( 85 89  85 8(  +  +  ,  %  86 89  %  86  8( 86 89  86 8(  JB !, ( =  1 2  HCC + HDD + HCC − HDD cos 2N(!, () + 2HCD sin 2N(!, ()  54  6. Color Image Processing  Spring 2023  6.7 Using color in image segmentation\n",
            "0.72: processing\n",
            "0.72: Color Image Processing\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "What is the difference image processing?\n",
            "\n",
            "0.88: Color Image Processing\n",
            "0.84: Figure 6.38 Image smoothing with a 5×5 averaging kernel. (a) Result of processing each RGB component image. (b) Result of processing the intensity component of the HSI image and converting to RGB. (c) Difference between the two results.  47  6. Color Image Processing  Spring 2023  6.6 Color image smoothing and sharpening  } Color image sharpening  } Ex. 6.13 (image sharpening using the Laplacian)  Q% M %, & =  Q%*(%, &) Q%+(%, &) Q%,(%, &)  Figure 6.39 Image sharpening using the Laplacian. (a) Result of processing each RGB channel. (b) Result of processing the HSI intensity component and converting to RGB. (c) Difference between the two results.  48  6.\n",
            "0.82: processing\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for query_result in res.json()['results']:\n",
        "    query = query_result['query']\n",
        "    answers = []\n",
        "    scores = []\n",
        "    for result in query_result['results']:\n",
        "        answers.append(result['text'])\n",
        "        scores.append(round(result['score'], 2))\n",
        "    print(\"-\"*70+\"\\n\"+query+\"\\n\\n\"+\"\\n\".join([f\"{s}: {a}\" for a, s in zip(answers, scores)])+\"\\n\"+\"-\"*70+\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsQCGl0EtXPq"
      },
      "source": [
        "The top results are all relevant as we would have hoped. With that we've finished. The retrieval app API can be shut down, and to save resources the Pinecone index can be deleted within the [Pinecone console](https://app.pinecone.io/)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "orig_nbformat": 4,
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "07f9b96cde584dc99bcc7c0fc95d75e7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "090ab955bf40499d834929e9bf8da0bb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13f11b0a05d7466d9edd84f06e56b687": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "159419e6e6024fc5891ef97e43dce6a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "184f5290a65649f59a99b649a3fcc8af": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2550ea896b3a4b39bcb37b374954850c",
              "IPY_MODEL_b136affd421f4c46aae0c9f1cc6f503f",
              "IPY_MODEL_30ce466cba1f45458cfca60be046afa9"
            ],
            "layout": "IPY_MODEL_b9d6b9723b6f4e63bc26aba581fe047a"
          }
        },
        "22b630a8792742db859dfcd03026d1b0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2550ea896b3a4b39bcb37b374954850c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_67a601ee00f741d29067bf6640012328",
            "placeholder": "​",
            "style": "IPY_MODEL_a7f21a216faa48ec83d8cfdb3e059daf",
            "value": "Generating train split: "
          }
        },
        "274feec9651545b3a692d028c31fcac5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30ce466cba1f45458cfca60be046afa9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_833bef95bd7741babb7424ff635d6c92",
            "placeholder": "​",
            "style": "IPY_MODEL_94789f4dbdb74046940ccc9594cf83ce",
            "value": " 0/0 [00:00&lt;?, ? examples/s]"
          }
        },
        "3966e35556e94afd9dbed920ab8bbf85": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3a663fb464b042cd95173d68762a7d9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3cff9800fbee4d73a66d7756c6eff7ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07f9b96cde584dc99bcc7c0fc95d75e7",
            "placeholder": "​",
            "style": "IPY_MODEL_d0c9ce854ea84ddd9653e207cd8515c4",
            "value": "Downloading data: 100%"
          }
        },
        "3f563339024a4287bbbb20f85b313f7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_090ab955bf40499d834929e9bf8da0bb",
            "max": 23,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_159419e6e6024fc5891ef97e43dce6a7",
            "value": 23
          }
        },
        "476958e0b9e94a06a96906cbcd61f2df": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f09b7d4613454798b6d1ec2a5fe53e7e",
            "max": 2757737,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_87a5ab1eedc74924847ad33c1898790d",
            "value": 2757737
          }
        },
        "592cb1e2e05a401da82a041a5ca14cf4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "59aa9111efc44cad8cc4cdb039d8a183": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df8d4737c49347de98add5c2fc426f35",
            "placeholder": "​",
            "style": "IPY_MODEL_13f11b0a05d7466d9edd84f06e56b687",
            "value": "100%"
          }
        },
        "6277c6e76ea34522b4b4231194cb5634": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6359a65a6d934899b67139e72d5991a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2a6a319a1024535ba4a45f84fc4ec46",
            "placeholder": "​",
            "style": "IPY_MODEL_3966e35556e94afd9dbed920ab8bbf85",
            "value": " 2.76M/2.76M [00:00&lt;00:00, 6.27MB/s]"
          }
        },
        "64380a778acb473685733f2969b64c1e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "668123cd76bd4a8c919166d605270afe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67a601ee00f741d29067bf6640012328": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ab224fc238f4de2865fb08566df6fea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_59aa9111efc44cad8cc4cdb039d8a183",
              "IPY_MODEL_3f563339024a4287bbbb20f85b313f7c",
              "IPY_MODEL_e83fc8770c3d4a9a9e0c2d1198db66d9"
            ],
            "layout": "IPY_MODEL_22b630a8792742db859dfcd03026d1b0"
          }
        },
        "6b4cd53d7bba4c7f8337e497237eaf30": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d66b2d8b37944a159371cec81bd1e4a1",
            "placeholder": "​",
            "style": "IPY_MODEL_7ee774773b2646159734e5d87731c178",
            "value": " 1/1 [00:00&lt;00:00, 35.02it/s]"
          }
        },
        "771af082b5874ecea8b51759ff5cbb52": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c0b48d09e3845439b6f17e56324ebae": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ee774773b2646159734e5d87731c178": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "825e264842644347a526bf5dafea9096": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "833bef95bd7741babb7424ff635d6c92": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87a5ab1eedc74924847ad33c1898790d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "87c3a27ea0f34db1be58be50122cbf77": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90e8fd7e27514c108bc81b39061bf81e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bed6a08879154cb19d5fed25c8abe332",
            "value": 1
          }
        },
        "90e8fd7e27514c108bc81b39061bf81e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94789f4dbdb74046940ccc9594cf83ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "97bea5162b9f4ee082f2dd2cd411ce89": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1cedb6c31104cd48214a8ec838a1616": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3cff9800fbee4d73a66d7756c6eff7ab",
              "IPY_MODEL_476958e0b9e94a06a96906cbcd61f2df",
              "IPY_MODEL_6359a65a6d934899b67139e72d5991a6"
            ],
            "layout": "IPY_MODEL_bacca9fafb3b464aa09f359ca707477f"
          }
        },
        "a2a6a319a1024535ba4a45f84fc4ec46": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7f21a216faa48ec83d8cfdb3e059daf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ae16059db75448ebb78617bbeb4cf739": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b136affd421f4c46aae0c9f1cc6f503f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "info",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_592cb1e2e05a401da82a041a5ca14cf4",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d724f8048f684a0ba96811b8d7da3154",
            "value": 1
          }
        },
        "b3ac85e446684f3da8fb63dbcba1fa20": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_825e264842644347a526bf5dafea9096",
            "placeholder": "​",
            "style": "IPY_MODEL_3a663fb464b042cd95173d68762a7d9d",
            "value": " 1/1 [00:00&lt;00:00,  2.17it/s]"
          }
        },
        "b6da9e973dd84c708dbf78d82fa0f966": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e00bc317b3d140c2ad208b358d593a02",
              "IPY_MODEL_87c3a27ea0f34db1be58be50122cbf77",
              "IPY_MODEL_6b4cd53d7bba4c7f8337e497237eaf30"
            ],
            "layout": "IPY_MODEL_668123cd76bd4a8c919166d605270afe"
          }
        },
        "b9d6b9723b6f4e63bc26aba581fe047a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "ba74b3d3c8cf460598ad4aeb8d0b32f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c0b48d09e3845439b6f17e56324ebae",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ae16059db75448ebb78617bbeb4cf739",
            "value": 1
          }
        },
        "bacca9fafb3b464aa09f359ca707477f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bed6a08879154cb19d5fed25c8abe332": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c3a4b108a23f444e847eb6735e7d35ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d0c9ce854ea84ddd9653e207cd8515c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d66b2d8b37944a159371cec81bd1e4a1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d724f8048f684a0ba96811b8d7da3154": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ddd67395d74d41a5a0fbd3208f904550": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df8d4737c49347de98add5c2fc426f35": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e00bc317b3d140c2ad208b358d593a02": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ddd67395d74d41a5a0fbd3208f904550",
            "placeholder": "​",
            "style": "IPY_MODEL_c3a4b108a23f444e847eb6735e7d35ce",
            "value": "Extracting data files: 100%"
          }
        },
        "e17fc50a411b4976919929cff7afa329": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f226a7168836423c9fa255805a2a961a",
              "IPY_MODEL_ba74b3d3c8cf460598ad4aeb8d0b32f7",
              "IPY_MODEL_b3ac85e446684f3da8fb63dbcba1fa20"
            ],
            "layout": "IPY_MODEL_64380a778acb473685733f2969b64c1e"
          }
        },
        "e83fc8770c3d4a9a9e0c2d1198db66d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97bea5162b9f4ee082f2dd2cd411ce89",
            "placeholder": "​",
            "style": "IPY_MODEL_6277c6e76ea34522b4b4231194cb5634",
            "value": " 23/23 [02:01&lt;00:00,  4.23s/it]"
          }
        },
        "f09b7d4613454798b6d1ec2a5fe53e7e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f226a7168836423c9fa255805a2a961a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_771af082b5874ecea8b51759ff5cbb52",
            "placeholder": "​",
            "style": "IPY_MODEL_274feec9651545b3a692d028c31fcac5",
            "value": "Downloading data files: 100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
